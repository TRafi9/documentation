{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Hey there, I'm Talha!","text":"<p>I'm a data engineer and full stack developer. I enjoy working with Python, Go (especially Go...), Docker, and those GCP Data engineering tools.</p>"},{"location":"#what-im-all-about","title":"\ud83d\ude80 What I'm All About","text":""},{"location":"#data-engineering","title":"Data Engineering","text":"<p>Working in GCP I create and containerise ETL logic in both Python and Go and launch them using DAGs on airflow with configuration to point to data sources/sinks.</p>"},{"location":"#full-stack-development","title":"Full Stack Development","text":"<p>I also work on both the frontend &amp; backend of different applications (although I'm more keen on backend dev), using Python/Go/NextJs and a touch of Kubernetes.</p>"},{"location":"#whats-this-website-for","title":"\ud83d\udcda What's This Website For?","text":"<p>This website is my little corner of the internet where I share all the things I've learned and built. It's like a digital scrapbook, but instead of embarrassing childhood photos, it's filled with code snippets and project demos! \ud83d\ude04</p> <p>Here's what you'll find:</p> <ul> <li> <p>Tutorials &amp; Guides \ud83d\udcdd: I share step-by-step guides on how to do some pretty neat stuff with data engineering, full stack development, and all the tech I work with. It's like having a cheat sheet for my coding problems!</p> </li> <li> <p>Projects \ud83d\udee0\ufe0f: I showcase the projects I've poured my heart and soul into. From the technologies I used to the challenges I faced, I spill all the juicy details. It's like a behind-the-scenes look at my coding journey!</p> </li> <li> <p>Blog \ud83e\udd14: This is where I let my thoughts run wild! I write about my experiences, lessons learned, and programming.</p> </li> </ul>"},{"location":"#lets-connect","title":"\ud83e\udd19 Let's Connect!","text":"<p>If you've got any questions, brilliant ideas, or just want to geek out about tech, hit me up! You can find my contact here!</p> <p>Thanks for swinging by! \ud83d\ude04</p>"},{"location":"needToLearn/","title":"Here's a page of all the stuff I want to learn/do","text":"<ul> <li>Go Concurrency</li> <li>API security best practices + Restful API design principles</li> <li>Event streaming (Pub/Sub / Kafka)</li> <li>More Kubernetes</li> </ul>"},{"location":"questions/","title":"questions","text":"<ol> <li>what is the command to list namespaces and select one as default one? can only find config related commands online</li> </ol>"},{"location":"todo/","title":"section on exposing frontend to public","text":"<p>check frontend-service yaml, no ingress needed for external IP as Loadbalancer provisioning will do this through the following command: apply frontend service then use external Ip and port exposed by service to see frontend: 159.65.212.242:3000</p>"},{"location":"todo/#configuring-cloudflare-domain-to-link-up-to-kubernetes","title":"configuring cloudflare domain to link up to kubernetes","text":"<ul> <li>have the loadbalancer public IP address and expose a port for main traffic, e.g. 443 for standard https traffic</li> <li>allow traffic from loadbalancer IP address through creating a dns record in cloudflare</li> <li>create and get origin server ssl/tls certs from cloudflare (will need these to marry up domain access to http)</li> <li>save cert and private key file locally e.g. tls.crt &amp; tls.key</li> <li>pass those values into a kubernetes secret of type tls, which expects a --cert and --key value:   <code>kubectl create secret tls cloudflare-tls-secret --cert=/path/to/file/tls.crt --key=/path/to/file/tls.key</code></li> <li>add the secret to the frontend ingress yaml manifest &amp; make sure to add it under tls heading too.</li> <li>map the ports using an ingress file to point to your port exposed by loadbalancer e.g. 443</li> </ul>"},{"location":"todo/#finish-off-things-learned","title":"finish off things learned","text":"<ul> <li>storing secret credentials outside of code and calling through env vars</li> <li> <p>setting up user login experience which includes parsing form data coming from frontend, uploading to DB, verifying emails and authenticating user during login</p> </li> <li> <p>blocking user login if user has not verified email</p> </li> <li> <p>how to send error responses from the backend and display those responses on the frontend for user</p> </li> <li> <p>kubernetes - done</p> </li> <li>JWT tokens - done</li> <li>login system - done</li> <li>crud calls in Go - done</li> <li> <p>password encryption - done</p> </li> <li> <p>CURRENTLY - learning implementing JWT Auth and storing JWT securely in cookies - passing back and forth through authorization header   what this project demonstrates</p> </li> <li> <p>understanding of writing sql statements for postgres</p> </li> <li> <p>FIND OUT - why api calls needed to be moved onto server side rather than client side in nextjs to access it by kubernetes? also move all calls to serverside</p> </li> </ul>"},{"location":"DataStructures_Algorithms/Binary_search/","title":"Binary search","text":""},{"location":"DataStructures_Algorithms/Binary_search/#big-o-notation-and-efficiency","title":"Big O notation and efficiency","text":"<p>Binary search will work on any sorted list of elements. It has a time complexity of O(Log n) (base of 2). This is because the number of required steps it takes to find the element is proportional to the logarithm of n.</p> <p>For example, take a list of 64 items. In the worst case scenario, the binary search will have to divide the list 6 times before it reaches its answer. log64 = 6. So <code>n</code> can essentially be revised as the number of items in the list, in order to find the worst case scenario of dividing the algorithm.</p> <p><code>2x2x2x2x2x2 = 64</code></p>"},{"location":"DataStructures_Algorithms/Binary_search/#implementation","title":"Implementation","text":"<pre><code>import math\n\nsortedList = []\ni = 0\nwhile i &lt; 50:\n    sortedList.append(i * 2)\n    i += 1\nprint(sortedList)\n\n# Implement binary search on sortedList\n\ntarget = 45\n\ndef binarySearch(target, sortedList):\n    startIndex = 0\n    endIndex = len(sortedList) - 1\n    middleIndex = math.ceil((startIndex + endIndex) / 2)\n\n    while startIndex &lt;= endIndex:\n        if target == sortedList[middleIndex]:\n            return (middleIndex, sortedList[middleIndex])\n        elif target &gt; sortedList[middleIndex]:\n            startIndex = middleIndex + 1\n        elif target &lt; sortedList[middleIndex]:\n            endIndex = middleIndex - 1\n        middleIndex = math.ceil((startIndex + endIndex) / 2)\n\n    return -1\n</code></pre>"},{"location":"blockchain/blockchainBasics/","title":"Blockchain basics","text":""},{"location":"blockchain/blockchainBasics/#hashes-nonces-mining","title":"Hashes, Nonces &amp; Mining","text":"<p>A hash is a fingerprint of digital data.</p> <p>In crypto, the hash is normally set to a certain difficulty, e.g. for mining the block, you have to find a hash for the block that starts with 4 zero's (the more zeros the more difficult it is to find), the miner will iterate over the nonce number to find a value that fits with the rest of the blocks data, so that when the hash of the whole block is taken, it starts with 4 zeros. The miner that finds this value first is normally rewarded with a small bit of crypto.</p>"},{"location":"blockchain/blockchainBasics/#maintaining-chain-heritage-democracy","title":"Maintaining chain heritage (Democracy!)","text":"<p>Because each blocks data contains the hash of the previous block, if a previous block is changed, it will change the hash of that block and therefore the <code>previous hash</code> value in the next block after it will change, this will in turn cause the hash value of that block to change too (and therefore not meet the conditions), and all blocks going ahead.</p> <p>For someone to try to change the value of a blockchain in the middle of the chain, they will have to compute all the blocks going forward too, this will make the chain look valid. However there are some reasons this is not feasible.</p> <ol> <li>Blockchain works as a distributed trust system which means the chain that is considered the truth is the one that is the longest + most common amongst the network.</li> <li>To meet the above conditions, one will have to own 51%+ of the network so they can have the most common chain and also this is super computationally expensive as they would have to be mining all the blocks going forward on their 51% owned network to keep up the facade.</li> </ol>"},{"location":"blockchain/blockchainBasics/#publickeys-private-keys-verification","title":"PublicKeys, Private Keys &amp; Verification","text":"<p>With public and private keys you can sign messages and people can verify that it was you who signed them.</p> <p>When you write a message you can use your <code>private key</code> to sign it. This will in turn create a message signature, which will be a unique string that is generated based on the message you signed + the private key you signed it with.</p> <p><code>PrivateKey(Message) = messageSignature</code></p> <p>The public key of the person who signed the message is available to all users, along with the message signature. Using these + the message, anyone can verify that the Data that has been sent, was generated by the person that held the private key.</p> <p>Using the public key of the person &amp; the signature, any user can verify if the message has been signed by the person holding the private key, the function will only return true/false.</p> <p><code>PublicKey&amp;Signature(Message) = True/False</code></p>"},{"location":"blockchain/blockchainBasics/#the-makeup-of-a-block","title":"The makeup of a block","text":""},{"location":"blockchain/blockchainBasics/#how-transactions-enter-a-block","title":"How transactions enter a block","text":"<p>Before moving onto how PoW works, its important to understand what makes up a block and how they run.</p> <p>Blocks are created by the miners of the network, here's how.</p> <ol> <li> <p>Transaction propagation When a user initiates a transaction, they send the data to the network. These transactions are broadcasted to the nodes (computers) in the network.</p> </li> <li> <p>Transaction Validation Nodes in the network receive transactions and verify their validity by checking factors such as if the user has sufficient funds, if the formatting is correct, if it includes the correct digital signature.</p> </li> <li> <p>Transaction Pool Those transactions are then added to a place called the <code>transaction pool</code> or sometimes <code>metapool</code>. This pool is formed of transactions that are still pending &amp; therefore not included in any blocks.</p> </li> <li> <p>Block Formation Miners then periodically pick up transactions from the pool and group them into blocks. They typically prioritise transactions based on things such as transaction fees, transaction size, age of transaction etc.</p> </li> <li> <p>Proof of Work Once a miner has selected transactions to be included in a block they begin trying to find a valid block hash using the <code>Proof of Work</code> algorithm, which involves repeatedly hashing the blocks header with a nonce and adding iterating the nonce value till it finds a valid hash that meets the difficulty level.</p> </li> <li> <p>Block Addition Once the miner finds a valid block hash, they broadcast the block to the network. Other nodes in the network validate the block and, if it is deemed valid by the majority of the network, the transactions included in the block are considered confirmed as the block is added to the chain which is immutable.</p> </li> </ol>"},{"location":"blockchain/blockchainBasics/#elements-of-a-block","title":"Elements of a Block","text":""},{"location":"blockchain/blockchainBasics/#block-header","title":"Block header","text":"<p>The block header consists of the following things: <code>Version number</code> of the blockchain protocol being used, allowing for upgrades and improvements to the protocol over time.</p> <p><code>Previous Block Hash</code> which allows it to link to the previous block.</p> <p><code>Merkle Root</code> which is a hash of all transactions included in the block, it provides an efficient way to verify the integrity of all the transactions in the block.</p> <p><code>Timestamp</code> which records the time at which the block was created.</p> <p><code>Difficulty target</code> which represents current difficulty level for mining the block.</p> <p><code>Nonce</code> - the value in which the miners vary to find a valid hash for the Proof of work step.</p>"},{"location":"blockchain/blockchainBasics/#block-body","title":"Block body","text":"<p>The block body contains all the transactions in the block, it may also contain the full merkle tree.</p>"},{"location":"blockchain/blockchainBasics/#proof-of-work","title":"Proof of work","text":"<p>The proof of work is defined as a blockchain concencus mechanism which uses computing power to verify crypto transactions and add them to the blockchain.</p> <p>In essence miners compete with each other to add a block to a blockchain, and they compete by trying to be the first to find a solution to a computationally intensive problem that has been broadcasted to the network. The first miner to solve this problem gets to add the block to the chain and the other nodes on the network can verify that it is a valid solution and therefore can confirm that the chain with the new block on it is the valid chain as long as the majority of nodes agree.</p>"},{"location":"blockchain/blockchainBasics/#how-proof-of-work-algorithm-tends-to-work","title":"How proof of work algorithm tends to work","text":"<p>The first thing you need to know about this concept is that a <code>block</code> tends to be made out of a few things, the <code>data</code> which tends to hold the transactions, the <code>previous hash</code> of the last block and a value called a <code>nonce</code></p>"},{"location":"csharp/Interfaces_and_classes/","title":"Interfaces in C","text":"<p>Interfaces are used to declare a set of members (methods, properties, etc.) that an implementing class must provide. When you define an interface, you specify the methods and properties it must have, but you do not define the implementation. This means that when a class implements an interface, it must define what those methods actually do. Interfaces usually start with the letter <code>I</code> (e.g., <code>ILogger</code>).</p>"},{"location":"csharp/Interfaces_and_classes/#why-use-interfaces","title":"Why Use Interfaces?","text":"<p>Interfaces are beneficial for several reasons:</p> <ul> <li>Decoupling: They allow you to separate the declaration of members (methods, properties, etc.) from the implementation details. This separation helps reduce dependencies and makes it easier to change implementations without affecting other parts of the code.</li> <li>Abstraction: Interfaces provide a way to work with different implementations. For example, you can define a logger interface (<code>ILogger</code>) that can be implemented as a database logger or a local file logger. The consuming code only needs to work with the <code>ILogger</code> interface and does not need to know the underlying implementation details. This abstraction is typically set up during configuration (e.g., in a startup file).</li> </ul>"},{"location":"csharp/Interfaces_and_classes/#example","title":"Example","text":"<p>The following example demonstrates how to define and implement an interface in C#:</p> <pre><code>public interface ILogger\n{\n    void LogMessage(string message);\n}\n\npublic class FileLogger : ILogger\n{\n    public void LogMessage(string message)\n    {\n        // Write the message to a file\n        System.IO.File.AppendAllText(\"log.txt\", message + Environment.NewLine);\n    }\n}\n\npublic class DatabaseLogger : ILogger\n{\n    public void LogMessage(string message)\n    {\n        // Save the message to a database\n        Console.WriteLine(\"Logged to database: \" + message);\n    }\n}\n</code></pre> <p>In the above example:</p> <ul> <li><code>ILogger</code> is an interface that declares a <code>LogMessage</code> method.</li> <li><code>FileLogger</code> and <code>DatabaseLogger</code> are two different classes that implement the <code>ILogger</code> interface with their own definitions of the <code>LogMessage</code> method.</li> </ul>"},{"location":"csharp/Interfaces_and_classes/#usage-example","title":"Usage Example","text":"<p>To instantiate and use one of these classes through the <code>ILogger</code> interface, you can do the following:</p> <pre><code>public interface ILogger\n{\n    void LogMessage(string message);\n}\n\npublic class DatabaseLogger : ILogger\n{\n    public void LogMessage(string message)\n    {\n        Console.WriteLine(\"Logged to database: \" + message);\n    }\n}\n\npublic class Program\n{\n    public static void Main()\n    {\n        // Create an instance of DatabaseLogger and use it through the ILogger interface\n        ILogger myDBLogger = new DatabaseLogger();\n\n        // Log a message using the DatabaseLogger's implementation of LogMessage\n        myDBLogger.LogMessage(\"Hello, world!\");\n    }\n}\n</code></pre> <p>In this example:</p> <ul> <li><code>myDBLogger</code> is created as an instance of <code>DatabaseLogger</code> but is referred to using the <code>ILogger</code> interface.</li> <li>This allows for flexibility; you can later swap out <code>DatabaseLogger</code> for another implementation (e.g., <code>FileLogger</code>) without changing the rest of your code.</li> </ul>"},{"location":"csharp/dependency_injection/","title":"Dependency Injection","text":"<p>Dependency injection is a design pattern used to loosely couple components. It allows you to pass dependencies (e.g. services, classes or objects) into a class without having the class create the dependency itself.</p>"},{"location":"csharp/dependency_injection/#why-use-dependency-injection","title":"Why use dependency injection?","text":"<ol> <li> <p>Loose Coupling: Classes are not tightly bound to specific implementations of their dependencies, making them easier to modify, replace, or test.</p> </li> <li> <p>Testability: You can pass mock or stub dependencies during unit testing</p> </li> <li> <p>Flexibility: You can switch between different implementations of a dependency without changing the class that depends on it, e.g. If you have a logger, you can log to the console during local development, but have the logger log to a database in production.</p> </li> </ol>"},{"location":"csharp/dependency_injection/#example-1","title":"Example 1:","text":"<p>In the following example we have a logger that we can use dependency injection to pass either the database or the console version of the logger:</p>"},{"location":"csharp/dependency_injection/#step-1-define-the-logger-interface","title":"Step 1: Define the Logger Interface","text":"<p>We'll create an ILogger interface that defines a <code>Log</code> method. This interface will be implemented by different loggers.</p> <pre><code>public interface ILogger\n{\n    void Log(string message)\n}\n</code></pre>"},{"location":"csharp/dependency_injection/#step-2-implement-logger-classes","title":"Step 2: Implement logger classes","text":"<p>We'll create two classes that implement ILogger which will both log out differently</p> <pre><code>public class ConsoleLogger: ILogger\n{\n    public void Log(string message){\n        Console.WriteLine(\"blah blah blah\")\n    }\n}\n\npublic class DatabaseLogger: ILogger\n{\n    public void Log(string message){\n        Console.WriteLine($\"DatabaseLogger:{message}\")\n    }\n}\n</code></pre>"},{"location":"csharp/dependency_injection/#step-3-create-a-service-that-depends-on-ilogger","title":"Step 3: Create a service that depends on ILogger","text":"<p>Now we will create a <code>TransactionService</code> class that needs <code>ILogger</code> to log its activities. Notice how TransactionService does not care about which ILogger implementation its using, just the fact that it uses an ILogger.</p> <pre><code>public class TransactionService\n{\n    private readonly ILogger _logger\n\n    // Constructor injection: ILogger is passed in through a constructor\n    public TransactionService(ILogger logger)\n    {\n        _logger = logger\n    }\n    public void ProcessTransaction(string transactionDetails)\n    {\n        _logger.log($\"Transaction Details: {transactionDetails}:\")\n    }\n}\n</code></pre>"},{"location":"csharp/dependency_injection/#step-4-setup-dependency-injection","title":"Step 4: Setup Dependency Injection","text":"<p>Now that we have <code>TransactionService</code> setup, we can essentially choose either the console or the database logger for the <code>ProcessTransaction</code> method to use. We do this using dependency injection, where we can inject the type of logger it uses. We can do this through microsofts built in dependency inection library.</p> <ol> <li>Install the Microsoft.Extensions.DependencyInjection NuGet package if needed:</li> </ol> <pre><code>dotnet add package Microsoft.Extensions.DependencyInjection\n</code></pre> <ol> <li>Create a simple console application with the following code:</li> </ol> <pre><code>using System;\nusing Microsoft.Extensions.DependencyInjection;\n\nnamespace DependencyInjectionExample\n{\n    public class Program\n    {\n        public static void Main(string[] args)\n        {\n            // Step 5: Configure DI Container\n            var serviceProvider = new ServiceCollection()\n                .AddTransient&lt;ILogger, ConsoleLogger&gt;()    // Use ConsoleLogger as the implementation of ILogger\n                //.AddTransient&lt;ILogger, DatabaseLogger&gt;() // Uncomment this line to use DatabaseLogger instead\n                .AddTransient&lt;TransactionService&gt;()        // Register TransactionService\n                .BuildServiceProvider();\n\n            // Step 6: Use the Service with DI\n            var transactionService = serviceProvider.GetService&lt;TransactionService&gt;();\n            transactionService.ProcessTransaction(\"Order ID: 12345, Amount: $250.00\");\n\n            // Output will be:\n            // Processing transaction...\n            // ConsoleLogger: Transaction Details: Order ID: 12345, Amount: $250.00\n        }\n    }\n}\n</code></pre>"},{"location":"csharp/dependency_injection/#net-implementation-example","title":".NET implementation example","text":"<p>In .NET, dependencies are usually configured in <code>Program.cs</code> or <code>Startup.cs</code>, here is an example where it is setup in .NET but also uses an env variable to switch the logger type:</p>"},{"location":"csharp/dependency_injection/#step-1-define-the-logger","title":"Step 1 define the logger:","text":"<pre><code>public interface ILogger\n{\n    void LogMessage(string message);\n}\n</code></pre>"},{"location":"csharp/dependency_injection/#step-2-implement-the-interfaces","title":"Step 2 Implement the interfaces","text":"<pre><code>public class FileLogger : ILogger\n{\n    public void LogMessage(string message)\n    {\n        Console.WriteLine($\"File Logger: {message}\");\n    }\n}\n\npublic class DatabaseLogger : ILogger\n{\n    public void LogMessage(string message)\n    {\n        Console.WriteLine($\"Database Logger: {message}\");\n    }\n}\n</code></pre>"},{"location":"csharp/dependency_injection/#step-3-configure-dependencies-in-programcs","title":"Step 3 Configure dependencies in Program.cs","text":"<p>Here we use an env variable to decide on logger type</p> <pre><code>// Microsoft.AspNetCore.Builder namespace provides WebApplication\n\nvar builder = WebApplication.CreateBuilder(args);\n\n// Step 3.1: Determine the environment or configuration for logger selection.\nvar environment = builder.Environment;\n\n// Step 3.2: Register the appropriate logger implementation based on the environment.\nif (environment.IsDevelopment())\n{\n    // Use FileLogger for development\n    builder.Services.AddTransient&lt;ILogger, FileLogger&gt;();\n}\nelse\n{\n    // Use DatabaseLogger for non-development environments\n    builder.Services.AddTransient&lt;ILogger, DatabaseLogger&gt;();\n}\n\n// Step 3.3: Register the ReportGenerator class, which will use the injected logger.\nbuilder.Services.AddTransient&lt;ReportGenerator&gt;();\n\nvar app = builder.Build();\n\napp.MapGet(\"/\", (ReportGenerator reportGenerator) =&gt;\n{\n    reportGenerator.GenerateReport();\n    return \"Hello World!\";\n});\n\napp.Run();\n</code></pre>"},{"location":"csharp/dependency_injection/#step-4-inject-the-ilogger-into-reportgenerator","title":"Step 4: Inject the ILogger into ReportGenerator","text":"<pre><code>public class ReportGenerator\n{\n    private readonly ILogger _logger;\n\n    // The logger dependency is injected via the constructor.\n    public ReportGenerator(ILogger logger)\n    {\n        _logger = logger;\n    }\n\n    public void GenerateReport()\n    {\n        _logger.LogMessage(\"Report generated\");\n    }\n}\n</code></pre> <p>In the example above we use an env variable to determine which logger to implement.</p> <p>Alternatively we can use <code>appsettings.json</code> to determine the logger we use.</p>"},{"location":"csharp/dependency_injection/#step-1-add-a-setting-to-appsettingsjson","title":"Step 1 Add a setting to <code>appsettings.json</code>","text":"<pre><code>{\n  \"Logging\": {\n    \"LoggerType\": \"File\" // Change to \"Database\" for DatabaseLogger\n  }\n}\n</code></pre>"},{"location":"csharp/dependency_injection/#step-2-read-in-the-appsettings-file-into-main","title":"Step 2 Read in the appsettings file into main","text":"<pre><code>var builder = WebApplication.CreateBuilder(args);\n\n// Read configuration setting from appsettings.json.\nvar loggerType = builder.Configuration[\"Logging:LoggerType\"];\n\nif (loggerType == \"File\")\n{\n    builder.Services.AddTransient&lt;ILogger, FileLogger&gt;();\n}\nelse if (loggerType == \"Database\")\n{\n    builder.Services.AddTransient&lt;ILogger, DatabaseLogger&gt;();\n}\n\nbuilder.Services.AddTransient&lt;ReportGenerator&gt;();\n\nvar app = builder.Build();\n\napp.MapGet(\"/\", (ReportGenerator reportGenerator) =&gt;\n{\n    reportGenerator.GenerateReport();\n    return \"Logger is running!\";\n});\n\napp.Run();\n</code></pre>"},{"location":"csharp/testingWithSpecFlow/","title":"testingWithSpecFlow","text":"<p>Specflow is used to conduct behavioural testing, giving non-technical users to write testing scenarios in the format: Given situation X When I do X Then I expect X result</p> <p>Each of these steps are then associated with functions the technical teams write to test these scenarios.</p> <p>Feature files e.g. CreateSomething.Feature will include the simple wording written in gherkin syntax e.g.: Given situation X When I do X Then I expect X result</p> <p>The StepDefinitions files will then have each of those steps written out associated with functions e.g:</p> <p>StepDefinitions.Given.cs file will have: [Given(situation X)] // some function for setting up the situation</p> <p>StepDefinitions.When.cs file will have: [When(I do X)] // some function for executing the situation</p> <ol> <li>Handle Common Setup via Background    If the Given steps are the same or nearly identical across scenarios, you can use a Background section to avoid repetition:</li> </ol> <p>gherkin Copy code Background: Given the charity being uploaded contains the following fields: | Charity Id | Parent Charity Id | Is National | Charity Name | Logo Uri | Charity Latitude | Charity Longitude | Postcode | | 21ea65e5-e144-6b5f-cb1f-bfcf949faa1c | | false | Borer and Sons | | 37.051 | 156.4469 | PT8 6UC | Then adjust scenarios as needed:</p> <p>gherkin Copy code Scenario: Add charity with valid fields returns 201 Created When I make a request to upload the new charity Then I should get a 201 created response</p> <p>Scenario: Add charity with invalid field types returns 400 Bad Request Given the charity data has invalid field types | Is National | | yes | When I make a request to upload the new charity Then I should get a 400 bad request response</p>"},{"location":"csharp/proC%23dotNet/dotNet_benefits/","title":"Benefits of .NET","text":"<p>.NET is a framework that supports multiple programming languages, such as C#, F#, and VB.NET. C# and F# are the primary languages used for ASP.NET Core.</p>"},{"location":"csharp/proC%23dotNet/dotNet_benefits/#1-multi-language-support","title":"1. Multi-Language Support","text":"<p>.NET leverages the Common Intermediate Language (CIL), which allows programming languages to be compiled down to a common format. This enables multiple programming languages to be used within the same assembly since they are all compiled into the CIL.</p>"},{"location":"csharp/proC%23dotNet/dotNet_benefits/#2-comprehensive-base-library","title":"2. Comprehensive Base Library","text":"<p>.NET includes a comprehensive base class library that provides predefined types and components for building libraries and enterprise-level websites. One example is the BaseController class, which provides a ready-made controller class with its associated methods, reducing development time.</p>"},{"location":"csharp/proC%23dotNet/dotNet_benefits/#3-jit-compilation-just-in-time-compilation","title":"3. JIT Compilation (Just-In-Time Compilation)","text":"<p>.NET uses a JIT compiler (Just-In-Time compiler), also referred to as the Jitter in some contexts. The JIT compiler varies depending on the infrastructure it's running on. It compiles code optimally based on the system's resources:</p> <ul> <li>On a large server, it may leverage abundant memory.</li> <li>On smaller devices like iOS, it may optimize for lower memory usage and improved performance.</li> </ul>"},{"location":"csharp/proC%23dotNet/dotNet_benefits/#4-strong-support-cycle","title":"4. Strong Support Cycle","text":"<p>.NET has a robust support cycle from Microsoft, with Long-Term Support (LTS) releases. LTS versions are supported for:</p> <ul> <li>3 years after the initial release.</li> <li>1 year of maintenance support after the subsequent LTS release.</li> </ul> <p>This ensures that businesses and developers can rely on stable and secure releases for extended periods.</p>"},{"location":"csharp/proC%23dotNet/dotNet_benefits/#5-common-type-system-and-common-language-specification","title":"5. Common Type System and Common Language Specification","text":"<p>.NET uses a Common Type System (CTS) and Common Language Specification (CLS) to ensure compatibility across languages.</p> <ul> <li>Common Type System (CTS) defines the types that .NET can support, but not all languages may support all types within this system.</li> <li>Common Language Specification (CLS) specifies which types within the CTS are supported by all .NET languages. </li> </ul> <p>If your application restricts itself to using only the types defined by the CLS, your code will be compatible and can be compiled into CIL even when integrating multiple languages. This provides seamless integration and ensures cross-language compatibility in .NET applications.</p>"},{"location":"csharp/proC%23dotNet/dotNet_benefits/#6-managed-vs-unmanaged-code","title":"6. Managed vs Unmanaged Code","text":"<p>Code created in C# that is CLS-compliant and targets the .NET framework is referred to as managed code. Managed code has several benefits:</p> <ul> <li>It can run on the .NET framework.</li> <li>It is platform-agnostic: code can be created on one operating system (e.g., Windows) and deployed on another (e.g., iOS) seamlessly.</li> </ul> <p>If the code does not comply with the CLS and therefore does not target the .NET framework, it is known as unmanaged code. Although unmanaged code can still be accessed and executed, it is locked to a specific development and deployment environment, making it less flexible compared to managed code.</p>"},{"location":"csharp/proC%23dotNet/CommonTypeSystem/cts/","title":"Common Type System","text":"<p>The Common Type System refers to a set of types that include:</p> <ul> <li><code>class</code></li> <li><code>interface</code></li> <li><code>structure</code></li> <li><code>enumeration</code></li> <li><code>delegate</code></li> </ul>"},{"location":"csharp/proC%23dotNet/CommonTypeSystem/cts/#interface","title":"Interface","text":"<p>An interface is a blueprint for its inheritors. For example, a class that inherits from an interface must implement its methods with the correct signature.</p>"},{"location":"csharp/proC%23dotNet/CommonTypeSystem/cts/#structure","title":"Structure","text":"<p>A structure is a lightweight class type with value-based semantics. It is typically best suited for modeling geometric or mathematical data.</p>"},{"location":"csharp/proC%23dotNet/CommonTypeSystem/cts/#example-a-c-structure-type","title":"Example: A C# Structure Type","text":"<pre><code>struct Point\n{\n    // Structures can contain fields.\n    public int xPos, yPos;\n\n    // Structures can contain parameterized constructors.\n    public Point(int x, int y)\n    {\n        xPos = x;\n        yPos = y;\n    }\n\n    // Structures may define methods.\n    public void PrintPosition()\n    {\n        Console.WriteLine(\"({0}, {1})\", xPos, yPos);\n    }\n}\n</code></pre>"},{"location":"csharp/proC%23dotNet/CommonTypeSystem/cts/#enumeration","title":"Enumeration","text":"<p>An enumeration is a construct that allows you to group name-value pairs. It is defined using the <code>enum</code> keyword.</p> <p>You can use it to enumerate a fixed set of values. By default, the values are assigned incrementally starting from zero.</p>"},{"location":"csharp/proC%23dotNet/CommonTypeSystem/cts/#example-implicit-values","title":"Example: Implicit Values","text":"<pre><code>enum DaysOfWeek\n{\n    Mon,   // 0\n    Tues,  // 1\n    Wed    // 2\n}\n</code></pre> <p>Or hard code them to point to something specifically e.g.</p> <pre><code>enum CharacterTypeEnum\n{\n    Wizard = 100,\n    Fighter = 200,\n    Thief = 300\n}\n</code></pre>"},{"location":"csharp/proC%23dotNet/CommonTypeSystem/cts/#delegate","title":"Delegate","text":"<p>A delegate is the .NET equivalent of a function pointer, but implemented as a class.</p> <p>Delegates are especially useful when you want to:</p> <ul> <li>Allow one object to forward a function call to another object.</li> <li>Support multicasting, i.e. forwarding a function call to multiple recipients.</li> <li>Enable asynchronous method invocation, i.e. invoking a method on another thread.</li> </ul> <p>They provide type safety and flexibility in scenarios like event handling and callbacks.</p>"},{"location":"goNotes/pointers/","title":"Pointers","text":"<p>Go is a pass by value language, meaning that when you pass a variable into a function, a copy of the variable is made, meaning that in some cases this can be memory inefficient.</p> <p>To tackle this, Go uses pointers and m</p> <p>This is where pointers and memory addresses come in, you can use pointers and memory addresses, which will allow you to declare a variable and update its value without creating a copy of it.</p> <p>The <code>&amp;</code> operator returns a memory address of a variable where as the <code>*</code> operator points to a memory address.</p>"},{"location":"goNotes/pointers/#memory-addresses-pointers","title":"Memory addresses &amp; pointers","text":"<pre><code>x := 42\nfmt.Println(\"Value of x:\", x)     // Prints: Value of x: 42\nfmt.Println(\"Address of x:\", &amp;x)  // Prints: Address of x: &lt;memory address&gt;\n</code></pre>"},{"location":"goNotes/pointers/#dereferencing-pointers","title":"Dereferencing pointers","text":"<pre><code>// Declare a variable 'x' of type int\nvar x int = 10\n\n// Declare a pointer variable 'ptr' of type *int\n// Assign the memory address of 'x' to 'ptr' using the '&amp;' operator\nptr := &amp;x\n\n// Print the value of 'ptr' (which is the memory address of 'x')\nfmt.Println(\"Value of ptr:\", ptr)  // Output: Value of ptr: &lt;memory address&gt;\n\n// Dereference 'ptr' to access the value stored at the memory address it points to\n// This will print the value of 'x' indirectly through the pointer\nfmt.Println(\"Value pointed to by ptr:\", *ptr)  // Output: Value pointed to by ptr: 10\n\n// Modify the value of 'x' indirectly through the pointer\n*ptr = 20\nfmt.Println(\"Updated value of x:\", x)  // Output: Updated value of x: 20\n</code></pre>"},{"location":"goNotes/pointers/#example-usage-of-functions-using-pointers","title":"Example usage of functions using pointers","text":"<p>Below is an example of how a function can be created to use pointers to save on memory, as opposed to passing in plain variables which would end up meaning a copy is made in memory.</p> <pre><code>func double(x *int) {\n    *x *= 2 // Modifying the value at the memory address pointed by x\n}\n\nfunc main() {\n    x := 5\n    fmt.Println(\"Before doubling:\", x) // Prints: Before doubling: 5\n    double(&amp;x)                          // Passing the address of x\n    fmt.Println(\"After doubling:\", x)  // Prints: After doubling: 10\n}\n</code></pre>"},{"location":"goNotes/HTTP/octet-stream/","title":"What is Octet-stream and where does it come from?","text":"<p>When sending a response in a HTTP request, Go will automatically set the following three headers <code>Date</code>, <code>Content-Length</code> &amp; <code>Content-Type</code>.</p> <p>The <code>Content-Type</code> header is set by Go sniffing the data in the response body using its underlying <code>http.DetectContentType()</code> function. This means that it will automatically set the content type if it is able to detect it. However, if it fails to detect it, it will set it to <code>Content-Type: application/octet-stream</code>.</p> <p>NOTE Go sometimes has trouble distinguishing between JSON and plaintext so you can get around this by setting the Content type yourself as follows:</p> <pre><code>w.Header().Set(\"Content-Type\", \"application/json\")\nw.Write([]byte(`{\"name\":\"Alex\"}`))\n</code></pre>"},{"location":"goNotes/HTTP/requests/","title":"Requests","text":"<p><code>*http.Request</code> is a parameter that is a pointer to a struct that holds information about the current request.</p> <p>When an HTTP server receives a request from a client it creates a <code>http.Request</code> object that represents that request. The object is then populated with information extracted from the incoming request, things such as request method, URL, headers &amp; body are extracted.</p> <p>Note that the body is represented as an <code>io.ReadCloser</code> object.</p> <p>After processing the request, the HTTP server can send back a response to the client usually using <code>http.ResponseWriter</code></p>"},{"location":"goNotes/HTTP/responseWriters/","title":"Writers","text":"<p><code>http.ResponseWriter</code> parameter provides methods for assembling a HTTP response and sending it to the user, e.g. WriteHeader, Write.</p> <pre><code>func home(w http.ResponseWriter, r *http.Request) {\n    w.Write([]byte(\"Hello from Snippet box\"))\n}\n\nfunc main() {\n    fmt.Println(\"hi\")\n\n    mux := http.NewServeMux()\n    mux.HandleFunc(\"/\", home)\n    log.Print(\"starting server on 4000\")\n\n    err := http.ListenAndServe(\":4000\", mux)\n    log.Fatal(err)\n\n}\n</code></pre>"},{"location":"goNotes/HTTP/serveMuxFixedPaths/","title":"How paths for handler functions work in serveMux","text":"<p>There are two different types of paths that go serveMux allows, fixed and subtree paths for handler functions. If a path ends in a trailing <code>/</code> or is just <code>/</code> Go treats it as a wildcard and matches everything after it e.g. <code>/view/</code> will match <code>/view/items</code>, basically <code>/view/*</code>.</p> <p>If the path in the handler does not end with a trailing <code>/</code> go will treat it as an absolute path and it will not act as a wildcard and therefore not match anything after the path stated, e.g. <code>/view</code>.</p>"},{"location":"kafka/notes/","title":"Kafka","text":"<p>Topics are streams of data, you can have multiple topics in each broker. Topics are split into partitions with each partition being ordered and each element in the partition gets an incremental ID called offset. offsets between partitions are not ordered. There are multiple brokers, each may contain a replica of your topic and the partitions, this is for fault tolerance. A kafka cluster is composed of multiple brokers (normally 3+)</p> <p>When a producer streams data to a topic, it can wait for acknowledgements if it is setup this way. There are 3 types:</p> <ul> <li>no acks - meaning producer wont wait for topic to reply with any acks, this is good for super low latency but can be risky</li> <li>1 ack - the producer waits for the topic in the leader broker to reply</li> <li>all acks - the producer waits for the lead broker to reply and all followers to also return an ack, the follower brokers are the ones that will replicate   the data of the lead broker topic as soon as the data lands in the lead broker, once it is replicated it will then reply with the ack.   With kafka, once you connect to one broker, you are connected to all brokers.   Data within topics are held in their partition temporarily for up to 1 week before being wiped.</li> </ul> <p>When you write data to a topic, it will go randomly to a partition in the topic, however if you provide a key it will go to a single partition that is assigned to the key, this allows you to keep ordering.</p>"},{"location":"kafka/notes/#kafka-replication","title":"Kafka replication","text":"<p>At any time only one broker can be a leader for a given partition, the other brokers will synchronize to it.</p> <p>Consumer will read in order for each partition, it only needs to specify the topic name and one broker to connect to and kafka will automatically take care of pulling data from the correct brokers, as once you are connected to one broker in kafka you are connected to all of them. A consumer will read both partition 1 and 2's data in parallel when it is consuming from the same topic, but in order per partition. </p>"},{"location":"kafka/notes/#consumer-groups","title":"Consumer groups","text":"<p>Consumers read data in consumer groups. Each consumer within a group reads from exclusive partitions, i.e. you cannot have 2 consumers reading from the same partition. You can also not have more consumers than partitions being read from, otherwise some consumers will be inactive.  </p>"},{"location":"kafka/notes/#consumer-offsets-when-a-consumer-dies-and-recovers","title":"Consumer offsets (when a consumer dies and recovers)","text":"<p>Kafka stores consumer offsets in a topic named <code>__consumer_offsets</code>. When a consumer has processed data it constantly commits offsets to that topic. This means that if the consumer dies, it can pick up the feed by continuing off from when the last offset was committed to the <code>__consumer_offsets</code> topic.</p>"},{"location":"kubernetes/basics/settingUpKubernetes/","title":"Kubernetes","text":"<p>Kubernetes is an open source system used to deploy, scale and manage containerised applications anywhere.</p> <p>It is great for the following reasons:</p> <ol> <li>It abstracts away infrastruture, as it handles compute/network/storage on behalf of your workloads.</li> <li>It has built in service health monitoring, restarting containers if they are unhealthy or fail, making sure you have high availability.</li> <li>It has built in commands to do a lot of heavy lifting that goes into application management, using <code>kubectl</code>.</li> </ol>"},{"location":"kubernetes/basics/settingUpKubernetes/#components-of-kubernetes","title":"Components of Kubernetes","text":""},{"location":"kubernetes/basics/settingUpKubernetes/#cluster","title":"Cluster","text":"<p>This is what all your kubernetes parts run in.</p>"},{"location":"kubernetes/basics/settingUpKubernetes/#namespaces","title":"Namespaces","text":"<p>Namespaces conceptually compartmentalise your resources into virtual clusters, that way you can allocate a set amount of resources to each namespace. For example, if you create a namespace for each department in your company, you can limit the amount of CPU/RAM that namespace has to distribute amongst the components inside it.</p> <p>Once you have a namespace, you can go ahead and add resources to it, such as variables using configMaps and secrets, or RAM/CPU set up from the resource part of the deployment manifest (the deployment.yaml).</p>"},{"location":"kubernetes/basics/settingUpKubernetes/#creating-a-namespace","title":"Creating a namespace","text":"<p><code>kubectl create namespace &lt;namespace name&gt;</code></p>"},{"location":"kubernetes/basics/settingUpKubernetes/#get-all-namespaces","title":"Get all namespaces","text":"<p><code>kubectl get ns</code></p>"},{"location":"kubernetes/basics/settingUpKubernetes/#configmaps","title":"ConfigMaps","text":"<p>ConfigMaps are objects used to store non-confidential variables as key : value pairs, pods can consume ConfigMaps as env variables/ comand line args or as config files in a volume. The advantage of ConfigMaps is that it allows you to decouple environment specific configurations from your application, making them easily portable and adjustable.</p>"},{"location":"kubernetes/basics/settingUpKubernetes/#creating-configmaps-from-kubectl","title":"Creating ConfigMaps from kubectl","text":"<p><code>kubectl -n &lt;namespace name&gt; create configmap &lt;configmap name&gt; --from-literal ENV_VAR_NAME=value</code></p>"},{"location":"kubernetes/basics/settingUpKubernetes/#list-out-configmaps","title":"list out configmaps:","text":"<p><code>kubectl -n &lt;namespace name&gt; get cm</code></p>"},{"location":"kubernetes/basics/settingUpKubernetes/#output-configmaps-as-yaml","title":"Output configmaps as yaml","text":"<p><code>kubectl -n &lt;namespace name&gt; get cm &lt;configmap name&gt; -o yaml</code></p>"},{"location":"kubernetes/basics/settingUpKubernetes/#secrets","title":"Secrets","text":"<p>Secrets are similar to configMaps except they are encrypted at rest. It is important to keep secrets separate from configMaps as you can apply rbac to give access to secrets to some people but not others.</p>"},{"location":"kubernetes/basics/settingUpKubernetes/#creating-a-secret-from-kubectl","title":"Creating a secret from kubectl","text":"<pre><code>kubectl -n &lt;namespace name&gt; create secret generic &lt;secret name&gt; \\\n--from-literal SECRET_KEY=secretValue \\\n--from-literal SECRET_KEY_2=secret_value_2\n</code></pre>"},{"location":"kubernetes/basics/settingUpKubernetes/#setting-secrets-in-deployment","title":"Setting secrets in deployment","text":"<p>Once you have created the secret, you can then pass the secret to your deployment yaml manifest like so:</p> <pre><code>    spec:\n      containers:\n        - name: tpm-backend\n          image: tpm-backend:0.0.2\n          imagePullPolicy: Never\n          ports:\n          - containerPort: 80\n          env:\n          - name: SECRET_NAME_IN_CONTAINER\n            valueFrom:\n                secretKeyRef:\n                    name: SECRET_NAME\n                    key: SECRET_KEY\n</code></pre>"},{"location":"kubernetes/basics/settingUpKubernetes/#list-all-secrets","title":"List all secrets","text":"<p><code>kubectl -n &lt;namespace name&gt; get secret</code></p>"},{"location":"kubernetes/basics/settingUpKubernetes/#deployments","title":"Deployments","text":"<p>We can use kubectl to apply our deployment manifest files. Deployment manifests are used to describe the specification of how we want to deploy our applications. e.g. what container, how much replicas we want, and the amount of resources we want to allocate to each deployment.</p> <p>Example of deployment manifest yaml file:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: mycontainer\n        image: myimage:tag\n        env:\n        - name: ENV_VARIABLE_FROM_CONFIGMAP\n          valueFrom:\n            configMapKeyRef:\n              name: myconfigmap\n              key: configKey\n        - name: ENV_VARIABLE_FROM_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: secretKey\n        ports:\n            - containerPort: 8080\n                protocol: TCP\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n</code></pre>"},{"location":"kubernetes/basics/settingUpKubernetes/#deploy-yaml-deployment-manifest","title":"Deploy yaml deployment manifest","text":"<p><code>kubectl -n &lt;namespace name&gt; apply -f &lt;deployment.yaml file&gt;</code></p>"},{"location":"kubernetes/basics/settingUpKubernetes/#view-deployed-pods","title":"View deployed pods","text":"<p>Get all pod names:</p> <p><code>kubectl -n &lt;namespace name&gt; get pods</code></p> <p>get specific pod:</p> <p><code>kubectl -n &lt;namespace name&gt; get pods/&lt;pod name&gt;</code></p>"},{"location":"kubernetes/basics/settingUpKubernetes/#services","title":"Services","text":"<p>Services are logical abstractions that provide stable endpoint to access our pods. Because pods are ephemeral, their IP addresses change when being spun up and down. Due to this, we cannot directly connect to them, as the connection will be unreliable, to counteract this issue, we use services, where we can use a stable IP address and port to point to specific pods via the app name defined in the service.</p> <p>Pods running different applications can communicate with each other through services as long as each application is being exposed by a service.</p> <p>Services are defined through a yaml manifest that can be applied.</p> <p>Example of backend service:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: tpm-backend\nspec:\n  type: ClusterIP\n  selector:\n    app: tpm-backend\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n</code></pre> <p>In the example above <code>port</code> is the port on which the service itself listens for traffic, while <code>targetPort</code> is the port to which traffic is forwarded to the backend pods, hence the targetPort needs to be the same port that the application's deployment yaml is exposing.</p> <p>By default, the type of service is <code>ClusterIP</code> if no other type is set, this is an internal IP address that provides an IP address for other components within the cluster to connect to, but does not allow connection from anything external.</p>"},{"location":"kubernetes/basics/settingUpKubernetes/#apply-service-yaml-manifest","title":"Apply service Yaml manifest","text":"<p><code>kubectl -n &lt;namespace name&gt; -f &lt;path to service yaml file&gt;</code></p>"},{"location":"kubernetes/basics/settingUpKubernetes/#list-all-services-in-namespace","title":"list all services in namespace","text":"<p><code>kubectl -n &lt;namespace name&gt; get svc</code></p> <p>running the above command will give us information for each service, including the clusterIP we can access our deployment on, as well as the port it exposes.</p>"},{"location":"kubernetes/basics/settingUpKubernetes/#applications-communicating-through-services-internally","title":"Applications communicating through services internally","text":"<p>If you have two different deployments running different images and each deployment exposes a service, the pods managed by these deployments can communicate with each other using their internal Cluster IP addresses and exposed ports.</p> <p>Here's how it works:</p> <p>1.You have two Deployments, each deploying Pods running different container images.</p> <p>2.Each Deployment exposes a Service, which creates a stable endpoint for accessing the Pods.</p> <p>3.The Pods can communicate with each other using the DNS name of the Service, which resolves to the internal Cluster IP address of the Service. They can use the internal Cluster IP address and exposed ports to send and receive data.</p> <p>Note that because services are logical abstractions, they can communicate across namespaces without any issues.</p>"},{"location":"kubernetes/basics/settingUpKubernetes/#ingress","title":"Ingress","text":"<p>Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. You can define an ingress API object through a yaml file.</p> <p>In an ingress API object you define the host that you expect traffic to be incoming from, and can define where you want to route that traffic, by defining which service you want to send it to, by giving it the name and port of the service.</p> <p>Example of ingress file:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tpm-backend-ingress\nspec:\n  rules:\n    - host: \"foo-bar.com\" # //hostname for the application e.g. my website, need to map domain name to node IP address 4:45 on video\n      http:\n        paths:\n          - pathType: Prefix\n            path: \"/tpm\"\n            backend:\n              service:\n                name: tpm-backend\n                port:\n                  number: 8080 # this points to the service port we are exposing\n</code></pre> <p>Defining and deploying an ingress resource is not enough, we need an ingress controller to actually run the ingress resource.</p>"},{"location":"kubernetes/basics/settingUpKubernetes/#ingress-controllers","title":"Ingress Controllers","text":"<p>Ingress controllers are used to run your ingress resource you have deployed through a yaml configuration. Controllers are not defaultly installed in your cluster, so you must install one manually, there are multiple resources to choose from, but we will keep it simple and use nginx.</p>"},{"location":"kubernetes/basics/settingUpKubernetes/#deploy-nginx-ingress-controller-through-kubectl","title":"deploy Nginx ingress controller through kubectl","text":"<p>The first thing is deploying your ingress controller to manage ingress as per your ingress manifest.</p> <p><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.0/deploy/static/provider/cloud/deploy.yaml</code></p> <p>The command above will create an nginx controller within its own namespace <code>ingress-nginx</code>. The ingress controller normally goes into its own namespace to make it easier to manage its resources and also separate its own resources so it has the capability to manage its own traffic.</p> <p>Even though the ingress controller is in its own namespace, by default it listens out for all the ingress resources deployed in our cluster across all namespaces.</p> <p>Once you have an ingress controller deployed, you can then route external network traffic to the ingress controller by routing traffic from your DNS to the ingress controllers <code>loadBalancer</code> IP address.</p>"},{"location":"kubernetes/basics/settingUpKubernetes/#get-loadbalancer-ip-address-of-ingress-controller","title":"Get LoadBalancer IP address of ingress controller.","text":"<p>This is the IP address you need to point external traffic to as an entry point to your cluster, i.e. you can set this IP address in the DNS settings of your cloudflare website. To find this IP address you can run the following command:</p> <p><code>kubectl -n &lt;ingress-controller-namespace&gt; get svc</code></p> <p>Note that this external IP address will be <code>pending</code> in the kind cluster, this is because LoadBalancer services are designed to use the load-balancer infrastructure your cloud provider offers, and since we aren't running in the cloud, there is none.</p> <p>This also means that when you apply your ingress manifest, there will be no address assigned, as the loadBalancer's external IP address will be the one assigned to your ingress. Once you have an ingress controller running that exposes a public IP address, you use that IP address to route traffic, e.g. point the DNS record to the ingress controller's IP address when creating a DNS record on cloudflare.</p>"},{"location":"kubernetes/basics/switchingClustersKubectl/","title":"Switching between clusters on kubectl","text":"<p>When working on multiple projects you may need to switch between kubernetes clusters. Use the commands below to do this.</p> <ol> <li><code>kubectl config get-contexts</code></li> <li><code>kubectl config use-context &lt;context name&gt;</code></li> </ol>"},{"location":"kubernetes/basics/switchingNamespacesKubectl/","title":"Switching default selected namespaces using kubectl","text":"<ol> <li><code>kubens</code> in terminal</li> <li>Select namespace</li> </ol>"},{"location":"kubernetes/cloudflare/connectingIngressToCloudflareDomain/","title":"Connecting Ingress To Cloudflare Domain","text":"<p>Once our application is up and running, accessible through a service with a load balancer type, and reachable via a public IP, we can integrate our Cloudflare domain. This means users will be redirected to our application upon visiting the domain.</p> <p>To do this, we need to perform the following steps:</p> <ol> <li> <p>Install an Ingress controller on our Kubernetes cluster.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml\n</code></pre> </li> <li> <p>Create and get secrets needed from Cloudflare and apply to your Kubernetes cluster. This allows Kubernetes to connect to your Cloudflare domain. See this guide (TODO: rewrite this page later).</p> </li> <li> <p>Create a YAML manifest for Ingress and point it to your Cloudflare domain and service. Make sure to add the annotations.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tpm-frontend-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream: \"true\"\n    nginx.ingress.kubernetes.io/auth-tls-secret: default/cf-orig\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"on\"\n    nginx.ingress.kubernetes.io/auth-tls-verify-depth: \"1\"\nspec:\n  ingressClassName: nginx\n  tls:\n    - hosts:\n        - tpm.talhaprojects.com\n      secretName: cf\n  rules:\n    - host: tpm.talhaprojects.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: tpm-frontend\n                port:\n                  number: 80\n</code></pre> </li> <li> <p>Deploy the YAML manifest and inspect it using <code>kubectl get ingress</code>. You should see an IP address available.</p> </li> <li> <p>Go to Cloudflare and into your project.</p> </li> <li> <p>Navigate to DNS -&gt; Records and create a new Type A record.</p> </li> <li> <p>Set the IPv4 address to the IP address you got from step 4, set proxied to ON and TTL Auto.</p> </li> <li> <p>Save DNS record and allow it to propagate.</p> </li> </ol>"},{"location":"kubernetes/cloudflare/nginxCertificates/","title":"Nginx ingress with Cloudflare origin SSL/TLS","text":"<p>I had an issue where my application was exposed through an nginx ingress type, but could not connect it to my cloudflare website. This is because cloudflare uses a self-signed SSL certificate to connect from their servers to yours. You will need to incorporate these to connect your ingress controller to cloudflare.</p>"},{"location":"kubernetes/cloudflare/nginxCertificates/#how-it-works","title":"How it works","text":"<p>Using the below diagram, the browser (you) connects to cloudflare, then cloudflare servers connect to your origin server (your webserver) and it uses its own SSL certificate to ensure it's encrypted, then it communicates back to cloudflare then to you.</p> <p></p> <p>We create a certificate that acts as a norma SSL certificate as far as cloudflare is concerned, however this one is self siged.</p> <p>We then download a pem file from Cloudflare which enabled client certificate authentication, which prevents people accessing the application if they don't have the certificate. Pretty much ensures access can only be made via cloudflare.</p>"},{"location":"kubernetes/cloudflare/nginxCertificates/#creating-ssl-certificate","title":"Creating SSL Certificate","text":"<p>note DNS Settings You must set your DNS setting for the domain (eg: <code>tpm.talhaprojects.com</code>) to use <code>proxied</code> otherwise this wont work</p> <p>In cloudflare, select your domain then navigate to SSL/TLS &gt; Origin server</p> <p>Enable Authenticated Origin pulls</p> <p>Click <code>Create Certificate</code></p> <p>Select Generate private key and CSR with cloudflare and select RSA (2048)</p> <p>Fill out the domain name</p> <p>You can select how long you want the certificate to be valid for, default is 15 years</p>"},{"location":"kubernetes/cloudflare/nginxCertificates/#create-the-files-locally","title":"Create the files locally","text":"<p>Once you've clicked <code>Create</code> from the previous screen, you are presented with 2 text boxes</p> <ul> <li>Origin Certificate</li> <li>Private Key</li> </ul> <p>Copy the Origin certificate in to a file called <code>cf.crt</code></p> <p>Copy the Private key in to a file called <code>cf.key</code></p>"},{"location":"kubernetes/cloudflare/nginxCertificates/#enable-strict-ssl","title":"Enable Strict SSL","text":"<p>Click Overview on the <code>**SSL/TLS**</code> navbar</p> <p>Under the top box, there is an option called <code>Full (strict)</code>, enable this</p> <p></p> <p>Download the Cloudflare Origin CA root certificate</p> <pre><code>wget https://developers.cloudflare.com/ssl/static/authenticated_origin_pull_ca.pem\n</code></pre>"},{"location":"kubernetes/cloudflare/nginxCertificates/#create-kubernetes-secrets","title":"Create kubernetes secrets","text":""},{"location":"kubernetes/cloudflare/nginxCertificates/#create-cloudflare-tls-secret","title":"Create cloudflare TLS Secret","text":"<p>This secret is used to ensure that requests are real and are coming from the cloudflare network</p> <pre><code>kubectl create secret generic cloudflare-tls-secret --from-file=ca.crt=./authenticated_origin_pull_ca.pem\n</code></pre>"},{"location":"kubernetes/cloudflare/nginxCertificates/#create-ssl-certificate","title":"Create SSL Certificate","text":"<pre><code>kubectl create secret tls cloudflare-origin-server --key cf.key --cert cf.crt\n</code></pre>"},{"location":"kubernetes/cloudflare/nginxCertificates/#configure-nginx-ingress","title":"Configure nginx ingress","text":"<p>Configure the below lines for your ingress.</p> <p>Make sure to chang <code>&lt;namespace&gt;</code> to the namespace where the secret is created</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream: \"true\"\n    nginx.ingress.kubernetes.io/auth-tls-secret: &lt;namespace&gt;/cloudflare-tls-secret\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"on\"\n    nginx.ingress.kubernetes.io/auth-tls-verify-depth: \"1\"\n  name: name\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: &lt;host-from-cloudflare&gt;\n      http:\n        paths:\n          - backend:\n              service:\n                name: &lt;service-name&gt;\n                port:\n                  number: &lt;service-port&gt;\n            path: /\n            pathType: Prefix\n  tls:\n    - hosts:\n        - &lt;host-from-cloudflare&gt;\n      secretName: cloudflare-origin-server\n</code></pre> <p>Once created, get the ingress IP</p> <pre><code>kubectl get ingress &lt;name&gt;\n</code></pre> <p>Output:</p> <pre><code>NAME            HOSTS     ADDRESS         PORTS     AGE\nbasic-ingress   *         203.0.113.12    80        2m\n</code></pre> <p>You should now set your DNS to use this IP address</p> <p>note : This page was sourced from here</p>"},{"location":"kubernetes/digitalOcean/pushingToDORegistry/","title":"Configuring Docker to Push to and Pull from DO Registry","text":"<p>To interact with your registry using the docker command-line interface (CLI), you need to configure docker using the DigitalOcean command-line tool, doctl.</p> <ol> <li>Install doctl CLI <code>brew install doctl</code></li> <li>Create an API key in DO.</li> <li>Run <code>doctl registry login</code> and login with API key, This command adds credentials to docker so that pull and push commands to your DigitalOcean registry will be authenticated.</li> </ol> <p>You can then use the docker tag command to tag your image with the fully qualified destination path, and docker push to upload it.</p>"},{"location":"kubernetes/digitalOcean/pushingToDORegistry/#commands","title":"Commands","text":"<p>Note that you will need to build your images using linux/amd64 to allow them to run on digital ocean instances.</p> <p><code>docker tag &lt;my-image&gt; registry.digitalocean.com/&lt;my-registry&gt;/&lt;my-image&gt;</code> <code>docker push registry.digitalocean.com/&lt;my-registry&gt;/&lt;my-image&gt;</code></p>"},{"location":"kubernetes/digitalOcean/pushingToDORegistry/#example","title":"Example","text":"<p><code>docker build --platform=linux/amd64 -t registry.digitalocean.com/tpm-containers-test/tpm-backend:0.0.05 .</code></p> <p><code>docker push registry.digitalocean.com/tpm-containers-test/tpm-backend:0.0.05</code></p>"},{"location":"kubernetes/general/Components/","title":"Kubernetes Component Documentation","text":""},{"location":"kubernetes/general/Components/#worker-nodes","title":"worker nodes","text":"<p>When you deploy kubernetes you get a cluster, within a cluster you will get a set of machines, which are called worker nodes, which will run your application.</p> <p>These worker nodes will run pods inside them, which are your containers at run time. A worker node can have multiple applications running on it.</p> <p>There are three components each worker node is comprised of:</p> <ul> <li>kubelet - An agent that runs on each worker node in a cluster, it makes sure that containers are running on a pod and are healthy, its primary responsibility is to manage the containers scheduled on the pods. It also is responsible to schedule to containers to run on the pods.   Because kubelets are responsible for starting the pods, they have to interface with both the worker node resources e.g. cpu/ram and the container.</li> <li>container runtime - each node needs a container runtime installed so containers are able to be run there, there are different types of container runtimes e.g. containered/ docker engine.</li> <li>kube-proxy - this part includes a quick explanation of services and how they interact with kube-proxy. Because pods are ephemeral, their connection details can and will change. Due to this, we need a stable way to connect to the pods that run our containers. Enter Services. Services are logical abstractions which create stable endpoints to connect to the pods, we can define which pods we want the endpoints to expose connectivity to by setting them in <code>selector</code> part of the yaml. Doing so will expose those pods to be connected to from anywhere in the cluster. When network packets from the client hit the service, the service then distributes the network packet to one of the nodes which hold a pod that is tagged with the same <code>selector</code> value e.g. our <code>tpm-backend</code>. It does this based on an algorithm as to not cause any networking bottlenecks, e.g. it might use round-robin to send different network packets to different nodes. Once the network packet is at the node, the kube-proxy then chooses which pod to distribute that data to (as we could have multiple pods spun up for the same application to increase availability), it will make this decision based on multiple factors e.g. pod health and load balancing algorithms in place.</li> </ul>"},{"location":"kubernetes/general/Components/#ingress","title":"Ingress","text":"<ul> <li>ingress controllers run the ingress yaml rules, they are not defaultly contained and started within the cluster, your preferred ingress controller (e.g. nginx) needs to be added to the cluster, so it can run the ingress rules defined</li> </ul>"},{"location":"kubernetes/general/debugging/","title":"Debugging","text":""},{"location":"kubernetes/general/envVarsInPodsForContainers/","title":"Getting ENV vars from pods to expose to containers","text":"<p>You can find the variables by doing the following.</p> <ol> <li>Find the pod you want with <code>kubectl get pods</code></li> <li>Exec into the pod using <code>kubectl exec -it &lt;pod name&gt; -- /bin/sh</code></li> <li>Type in <code>env</code> into terminal to bring up env vars</li> </ol> <p>From here you can see the env vars in the pods and can call them in your code on the server side. e.g. in python you could do:</p> <pre><code>import os\nos.getenv(ENV_VARIABLE_NAME)\n</code></pre> <p>and in javascript you can do: <code>const VAR = process.env.&lt;ENV VAR NAME&gt;</code></p> <p>e.g.</p> <pre><code>const TPM_BACKEND_SERVICE = process.env.TPM_BACKEND_SERVICE;\nconst TPM_BACKEND_SERVICE_PORT = process.env.TPM_BACKEND_SERVICE_PORT;\n</code></pre>"},{"location":"kubernetes/general/mappingAPICallsToServices/","title":"How to forward API calls from your application to your kubernetes containers.","text":"<p>In this scenario I have a frontend exposed to the public via an nginx ingress.</p> <p>Since my TPM backend is now sitting in a cluster, I need to reconfigure the API backend calls (made by the user clicking on stuff in the frontend) so that it points to the backend service that sits within my cluster.</p> <p>e.g. normally I call my backend from `localhost:8080/api/v1/something, but my backend now sits in the cluster and is exposed via a service.</p>"},{"location":"kubernetes/general/mappingAPICallsToServices/#how-to-reconfigure-the-backend-calls","title":"How to reconfigure the backend calls","text":"<ol> <li> <p>Exec into the pod running our container.    As long as we have a service setup properly we can find the ports and name of our service that we need to replace the api call with.    <code>kubectl exec -it &lt;pod name&gt;  -- /bin/sh</code> <code>kubectl exec -it tpm-frontend-8dfb5484b-z5v22 -- /bin/sh</code></p> </li> <li> <p>Find the env vars within the pod.    type <code>env</code> into the terminal after execing into the pod.</p> </li> <li> <p>From here you can see the env variables within the pod, we want to pull both the <code>SERVICE</code> and the <code>SERVICE PORT</code>.    In TPM the following variables are:    <code>TPM_BACKEND_SERVICE</code> and <code>TPM_BACKEND_SERVICE_PORT</code></p> </li> <li> <p>Call the environment variables into your code, as they should be available from the environment    JavaScript:    <code>ENV_VARIABLE = process.env.ENV_VARIABLE</code></p> </li> <li> <p>Rename the API calls to match the new service:</p> </li> </ol> <pre><code>- http://localhost:8080/api/v1/createUser\n+ http://$TPM_BACKEND_SERVICE:$TPM_BACKEND_SERVICE_PORT/api/v1/createUser\n</code></pre> <p>Note that calls need to be on server side for this to work, aka the call done by the frontend needs to execute on serverside.</p>"},{"location":"kubernetes/general/usefulCommands/","title":"useful commands","text":""},{"location":"kubernetes/general/usefulCommands/#describe-deployments","title":"Describe deployments","text":"<p>Gets deployment information</p> <p><code>kubectl describe deployment &lt;deployment name&gt;  -n &lt;namespace&gt;</code></p> <p>e.g.</p> <p><code>kubectl describe deployment tpm-backend  -n tpm</code></p>"},{"location":"kubernetes/general/usefulCommands/#retagging-existing-docker-image","title":"Retagging existing docker image","text":"<p><code>docker image tag tpm-backend:latest tpm-backend:0.0.1</code></p>"},{"location":"kubernetes/general/usefulCommands/#pushing-updated-code-to-kind-pod","title":"Pushing updated code to kind pod","text":"<ol> <li>rebuild your image with a new tag e.g.</li> </ol> <pre><code>-    docker build -t tpm-backend:0.0.1\n+    docker build -t tpm-backend:0.0.2\n</code></pre> <ol> <li>change deployment file to point image section to new tag</li> </ol> <pre><code>containers:\n        - name: tpm-backend\n-          image: tpm-backend:0.0.1\n+          image: tpm-backend:0.0.2\n</code></pre> <ol> <li>redeploy the deployment file using the following:</li> </ol> <p><code>kubectl apply -f &lt;path to deployment.yaml&gt;</code></p>"},{"location":"kubernetes/general/usefulCommands/#view-pod-status","title":"View pod status","text":"<p><code>kubectl get pods</code></p> <p><code>kubectl get pods/&lt;pod name&gt;</code></p>"},{"location":"kubernetes/general/usefulCommands/#view-container-logs-from-the-pod-its-running-in","title":"View container logs from the pod its running in","text":"<p><code>kubectl logs -f pod/tpm-backend-c5587bf9b-89pds -c cloud-sql-proxy</code></p> <p>-c stands for container <code>kubectl logs -f pod/&lt;pod name&gt; -c &lt;container name that is set in deployment.yaml&gt;</code></p>"},{"location":"kubernetes/general/usefulCommands/#get-continuous-logs-of-a-running-container-in-a-pod","title":"Get continuous logs of a running container in a pod","text":"<p><code>kubectl logs -f &lt;pod name&gt; -c &lt;container name&gt;</code></p>"},{"location":"kubernetes/google/mountingApplicationDefaultCredentials/","title":"Mounting application default credentials","text":"<p>You will need to add your service account / application default credentials to kubernetes if you want to connect and run things such as cloud sql proxy.</p> <ol> <li> <p>Download your <code>application_default_credentials.json</code> file from GCP.</p> </li> <li> <p>Create a secret that holds the contents of your credentials file.</p> </li> </ol> <p><code>kubectl create secret -n &lt;namespace name&gt; generic &lt;secret name&gt; --from-file=&lt;key name&gt;=&lt;path to application_default_credentials.json&gt;</code> <code>kubectl create secret generic cloudsql-credentials --from-file=service_account.json=application_default_credentials.json</code></p> <ol> <li> <p>In the deployment yaml, add the secret as a volume to the pod running the containers. The secret needs to be added at pod level first, and then can be mounted to a container.</p> </li> <li> <p>Mount the volume to the container within the deployment yaml file, do this using <code>volumeMounts</code> and define a path where you want it mounted within the container using <code>mountPath</code> (this path will be created if it doesn't already exist). Define the secret you are mounting using <code>name</code> which should be the same name as the secret that you set under <code>volumes</code>.</p> </li> <li> <p>For cloudsql you can pass the instance name and mounted credentials file under <code>args</code>. When secrets are mounted to containers, the key:value pairs are mounted, so you can directly pass them as the following:    <code>--credentials-file=/&lt;volume mount path&gt;/&lt;some key in secret&gt;</code></p> </li> </ol>"},{"location":"kubernetes/google/mountingApplicationDefaultCredentials/#example","title":"Example","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tpm-backend\n  labels:\n    app: tpm-backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tpm-backend\n  template:\n    metadata:\n      labels:\n        app: tpm-backend\n    spec:\n      volumes:\n        - name: cloudsql-credentials\n          secret:\n            secretName: cloudsql-credentials\n      containers:\n        - name: tpm-backend\n          image: tpm-backend:0.0.2\n          imagePullPolicy: Never\n          env:\n            - name: USER\n              valueFrom:\n                secretKeyRef:\n                  name: tpmsecrets\n                  key: USER\n        ....\n\n          ports:\n            - containerPort: 8080\n              protocol: TCP\n        - name: cloud-sql-proxy\n          image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.8.2\n          volumeMounts:\n            - mountPath: /secrets/\n              name: cloudsql-credentials\n              readOnly: true\n          env:\n            - name: USER\n              valueFrom:\n                secretKeyRef:\n                  name: tpmsecrets\n                  key: USER\n          args:\n            - starlit-booster-408007:europe-west2:the-productive-muslim\n            - --credentials-file=/secrets/service_account.json\n</code></pre>"},{"location":"kubernetes/kind/ingressOnKind/","title":"Exposing apps on kind through ingress","text":"<p>See: https://kind.sigs.k8s.io/docs/user/ingress/</p>"},{"location":"kubernetes/kind/ingressOnKind/#why-it-needs-a-special-setup","title":"Why it needs a special setup","text":"<p>Normally you can access your application through the external IP address provided by your ingress controller, which will forward data to the service attached.</p> <p>Because Kind runs locally, it does not have access to the external IP, as that is normally provisioned by the cloud provider, therefore you have to setup a specific nginx controller for your kind cluster, and also make sure extraPortMappings and node-labels are available on your cluster.</p> <p>In the example below, we create a cluster with <code>extraPortMappings</code> the <code>extraPortMappings</code> are used to allow the localhost to make requests to the ingress controller over ports 80 and 443. You can also use <code>node-labels</code> to allow the ingress controller to run on specific node(s) matching the label selector.</p>"},{"location":"kubernetes/kind/ingressOnKind/#deploying-the-cluster","title":"Deploying the cluster","text":"<pre><code>cat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\nEOF\n</code></pre>"},{"location":"kubernetes/kind/ingressOnKind/#deploying-the-nginx-ingress-controller","title":"Deploying the nginx ingress controller","text":"<p>We then need to apply the specific NGINX manifest for the kind cluster using: <code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml</code></p> <p>The manifests contains kind specific patches to forward the hostPorts to the ingress controller, set taint tolerations and schedule it to the custom labelled node.</p>"},{"location":"kubernetes/kind/ingressOnKind/#example-ingress-deployment","title":"Example ingress deployment","text":"<p>Once we have this setup, we can provision our frontend application to be exposed through localhost like so:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tpm-frontend-ingress\nspec:\n  rules:\n    - host: localhost\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: tpm-frontend\n                port:\n                  number: 3000\n</code></pre>"},{"location":"kubernetes/kind/kindCommands/","title":"Kind Commands","text":"<p>Kind is a tool used to run Kubernetes clusters inside Docker containers for local development and testing purposes.</p>"},{"location":"kubernetes/kind/kindCommands/#get-cluster-names-in-kind","title":"Get cluster names in kind","text":"<p><code>kind get clusters</code></p>"},{"location":"kubernetes/kind/kindCommands/#load-docker-images-into-kind","title":"Load docker images into kind","text":"<p><code>kind load docker-image &lt;Image name&gt; -n &lt;cluster name&gt;</code></p>"},{"location":"kubernetes/kind/kindCommands/#loading-docker-images-on-kind-clusters","title":"Loading docker images on kind clusters","text":"<ol> <li>Retag the image to something that isn't the tag <code>latest</code> as this causes issues in picking up the tag in kind. You need to make sure the deployment yaml points to this image in the image tag section too.</li> </ol> <p><code>docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]</code></p> <ol> <li> <p>Add the docker image into the cluster using the <code>kind</code> cli tool.    <code>kind load docker-image SOURCE_IMAGE[:TAG] -n &lt;CLUSTER NAME&gt;</code></p> </li> <li> <p>Make sure to update the deployment yaml to point to new image</p> </li> </ol> <p>Example</p> <p>1.</p> <p><code>docker tag tpm-backend:latest tpm-backend:0.0.1</code></p> <ol> <li><code>kind load docker-image tpm-frontend:0.0.1 -n kind</code></li> </ol> <p>3.</p> <pre><code>containers:\n        - name: tpm-backend\n-            image: tpm-backend:latest\n+            image: tpm-backend:0.0.2\n          imagePullPolicy: Never\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/overview/","title":"What is TPM?","text":"<p>If your lucky, its still up at tpm.talhaprojects.com</p> <p>TPM, short for \"The Productive Muslim,\" is a handy productivity tool that's inclusive for all users. It's a straightforward way to monitor your daily productivity without any fuss. Instead of complex metrics, TPM keeps it simple with a basic yes/no question. Because this question is boolean, it is really up to the user and their imagination to track whatever they want e.g. Instead of have you been productive? You can change it to Have you been happy? or Have you stayed on track with your diet so far?.</p>"},{"location":"myProjects/theProductiveMuslim/overview/#how-does-it-track-productivity","title":"How does it track productivity?","text":"<p>TPM operates in sync with Muslim prayer times, happening five times daily. It initiates a countdown to the next prayer and simply asks you to reflect on your productivity when it's time. You provide your answer, it's recorded, and then it's back to the countdown. With up to four check-ins available each day, TPM helps you stay on track. And if you ever want to review your progress, just take a glance at your submissions throughout the day, week, or month.</p>"},{"location":"myProjects/theProductiveMuslim/overview/#countdown-page-main-page","title":"Countdown page (main page)","text":"<p>This is the main page, which displays a countdown till the next prayer time </p>"},{"location":"myProjects/theProductiveMuslim/overview/#productivity-question-when-timer-expires","title":"Productivity question when timer expires","text":"<p>Once the timer runs out, the user gets asked if they've been productive between the last 2 prayers. This info is stored in the backend with prayer timestamps, which gives me the stats to be able to count the time spent being productive/unproductive and possibly visualise that in the future.</p> <p></p>"},{"location":"myProjects/theProductiveMuslim/overview/#users-stats-view","title":"Users stats view","text":"<p>Clicking <code>My Stats</code> in the nav bar takes the user to a page where they can see a simple visualisation of how productive they've been throughout different time periods. </p>"},{"location":"myProjects/theProductiveMuslim/overview/#why-prayer-times","title":"Why prayer times?","text":"<p>I chose to go with muslim prayer times because it breaks down the day into 5 segments and allows me to dynamically split up the day.</p>"},{"location":"myProjects/theProductiveMuslim/whyIchoseToMakeIt/","title":"Why I chose to make it (Learning)","text":"<p>Long story short I needed a project to help build my technical skills.</p> <p>I chose this project as it would help me learn and develop my skills in things such as Golang, kubernetes and API's, but turns out it helped me develop a lot more than that, you can find the deep details in the Skills used in Project section!</p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/JWT/","title":"JWTs","text":"<p>When creating the application, I had to figure out a way to make sure only users of the application were able to access certain TPM's API resources.</p> <p>Enter learning about JWT tokens, I created the following system.</p> <ol> <li>User logs in</li> <li>Server checks for valid credentials</li> <li>If credentials are valid, server sets a JWT token in the browser's cookies.</li> <li>Frontend then sends requests with JWT token bound in Header, which is then parsed by server to see if access is allowed to specific API routes.</li> </ol> <p>Here's some of TPM's code where it creates a JWT token and sets a 24 hour expiry time along with the userType.</p> <pre><code>claims := &amp;Claims{\n                UserType: \"user\",\n                User:     loginCredentials.UserEmail,\n                RegisteredClaims: jwt.RegisteredClaims{\n                    Issuer:    \"tpm\",\n                    ExpiresAt: jwt.NewNumericDate((time.Now().Add(24 * time.Hour))),\n                },\n            }\n\n            token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)\n\n            // Sign and get the complete encoded token as a string using the secret\n\n            tokenString, err := token.SignedString(hmacSecret)\n            if err != nil {\n                logger.Errorf(\"token.SignedString failed, err: \", err.Error())\n            }\n            cookie := &amp;http.Cookie{\n                Name:     \"jwt\",\n                Value:    tokenString,\n                Path:     \"/\",\n                HttpOnly: true,\n                SameSite: http.SameSiteStrictMode,\n                Secure:   true,\n                Expires: time.Now().Add(24 * time.Hour),\n            }\n\n            c.SetCookie(cookie)\n</code></pre> <ol> <li>Once the JWT token is set in the browser, I used it to protect specific backend API endpoints. I did this by adding middleware to specific backend API routes, which checks for a JWT token in any incoming API request, and checks to see if it is valid (signed by same hmac secret key, not expired, not tampered with etc). Here is the code snippet for that, it expects the token from the frontend to come through the header part of the request.</li> </ol> <pre><code>apiRestricted := e.Group(\"/api/v1/restricted\")\n\n// middleware to stop invalid or unauthorized jwt's from accessing these functions\napiRestricted.Use(echojwt.WithConfig(echojwt.Config{\n    SigningKey:  hmacSecret,\n    TokenLookup: \"header:Authorization\",\n}))\n\napiRestricted.GET(\"/getPrayerTimes/:dateValue\", func(c echo.Context) error {\n    return todayPrayerHandler(c, Pt, logger, hmacSecret)\n})\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/JWT/#why-i-did-this","title":"Why I did this?","text":"<ol> <li>Reduce likelyhood of external attacks on TPM api's e.g. DDOS</li> <li>Ability to restrict access further by parsing claims inside handler function, and checking if they are authorized to be making this call.</li> </ol>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/componentOverview/","title":"Component Overview","text":"<p>There's a bunch of different skills used in this project</p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/cookies/","title":"Cookies","text":"<p>While developing my application, I needed a way to make sure that users could only access specific backend api endpoints, depending on if the user was logged in or not. I stumbled upon cookies, which was great because I needed something where I could store the JWT token.</p> <p>Cookies are a great option for storing JWT tokens for the following reasons:</p> <ol> <li>They automatically get sent with every request from your client to the backend, so you don't have to manually send them with every request.</li> <li>Cookies are set by the server, so this allowed me a way to set the jwt token value from the backend, rather than exposing this code to the client side, making it easier to tamper with.</li> <li>JWT's are designed to be stateless, storing a JWT in a cookie allows the server to remain stateless.</li> <li>You can set expiry times on cookies, which you can choose how your server will respond to, most of the time the server will continue with the request and ignore the cookie if it is expired.</li> </ol>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/cookies/#setting-a-jwt-cookie-in-golang","title":"Setting a JWT cookie in Golang","text":"<p>Here is an example of how I set a cookie in golang.</p> <pre><code>token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)\n\n// Sign and get the complete encoded token as a string using the secret\n\ntokenString, err := token.SignedString(hmacSecret)\nif err != nil {\n    logger.Errorf(\"token.SignedString failed, err: \", err.Error())\n}\n\ncookie := &amp;http.Cookie{\n                Name:     \"jwt\",\n                Value:    tokenString,\n                Path:     \"/\",\n                HttpOnly: true,\n                SameSite: http.SameSiteNoneMode,\n                Secure:   true, // set true if using HTTPS\n\n                Expires: time.Now().Add(24 * time.Hour),\n            }\n\nc.SetCookie(cookie)\n</code></pre> <p>The code above sends a cookie from the golang server, using <code>SetCookie</code> this sets the jwt cookie in the <code>set-cookie</code> response headers when the API call has been made:</p> <pre><code>const jwtCookie = response.headers.get(\"set-cookie\");\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/cookies/#receiving-cookie-and-setting-it-in-browser","title":"Receiving Cookie and setting it in browser","text":"<p>One thing that threw me off was I was trying to make the api call on the client side, calling the backend directly on the clients webpage. This was not good as kubernetes required the calls to my backend to be server side. When I switched it to serverside (by creating an API call under the /api/ directory), I struggled to get the jwt to be set in my browser. This is because I was not doing the following things.</p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/cookies/#calling-the-cookie-from-the-set-cookie-response-header","title":"Calling the cookie from the set-cookie response header.","text":"<p>The response header was sent by the golang server with <code>c.SetCookie(cookieName)</code>. The code below allowed me to get the jwt cookie, which was held in set-cookie.</p> <pre><code>const jwtCookie = response.headers.get(\"set-cookie\");\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/cookies/#propogating-cookie-value-to-client-side-from-server-side","title":"Propogating Cookie value to client side from server side","text":"<p>Sending the cookie value in the header response of my NextJs serverside API call. The code below</p> <pre><code>res.setHeader(\"Set-Cookie\", jwtCookie);\n</code></pre> <p>When the client receives the response from the NextJs API route, it automatically recognises the Set-Cookie header and stores the cookie sent.</p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/cookies/#adding-credentialsinclude","title":"Adding credentials:\"include\"","text":"<p>Adding <code>credentials:\"include\"</code> flag in the API request. You need to set <code>credentials: \"include\"</code> in the fetch options when making cross-origin requests that require sending and receiving cookies.</p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/cookies/#receiving-and-setting-cookie-from-serverside-in-nextjs-example","title":"Receiving and setting cookie from serverside in NextJs Example","text":"<pre><code>      const response = await fetch(\"http://localhost:8080/api/v1/login\", {\n        method: \"POST\",\n        headers: {\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify({\n          userEmail,\n          userPassword,\n        }),\n        credentials: \"include\",\n      });\n\n      if (response.status == 200) {\n        const dataRes = await response.json();\n        const jwtCookie = response.headers.get(\"set-cookie\");\n        // Set the \"jwt\" cookie in the API route's response headers\n        if (jwtCookie) {\n          res.setHeader(\"Set-Cookie\", jwtCookie);\n        }\n\n        res.status(200).json(dataRes);\n      } else {\n        // If response is not successful, parse JSON response to get error message and status code\n        const errorResponse = await response.json();\n        res.status(500).json(errorResponse);\n      }\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/docker/","title":"Docker","text":"<p>3 different docker images were used in this project.</p> <ul> <li>Official image for SQL Cloud proxy</li> <li>My frontend image</li> <li>My backend image</li> </ul> <p>Heres an example of a docker file for my backend service that I was using for local testing</p> <pre><code>FROM golang:1.21\n\nWORKDIR /app\nCOPY . .\n\nRUN go build tpm\n\nEXPOSE 8080\n\nCMD [\"sh\", \"-c\", \"./tpm\"]\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/docker/#docker-compose","title":"Docker compose","text":"<p>I also made the use of docker compose files to speed up development and launch both the frontend and backend docker files at the same time, calling env variables from .env files so I could change configuration easily as I went along, here's an example:</p> <pre><code>version: \"3\"\n\nservices:\n  tpm-backend:\n    image: tpm-backend\n    build:\n      context: .\n      dockerfile: Dockerfile\n    env_file:\n      - .env\n    ports:\n      - \"8080:8080\"\n\n  tpm-frontend:\n    image: tpm-frontend\n    build:\n      context: ./frontend/tpm-frontend\n      dockerfile: Dockerfile\n    ports:\n      - \"3000:3000\"\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/docker/#passing-env-vars","title":"Passing env vars","text":""},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/docker/#locally","title":"Locally","text":"<p>There were 2 ways I was passing env vars in for local testing:</p> <ol> <li>Mounting env vars using the <code>env_file</code> flag in my docker-compose file</li> <li>When running docker images independantly, docker images were passed using env variables using the <code>-e</code> flag to pass env variables when the docker run command was executed, I used this to keep secrets outside my code base and pass them into the scripts.</li> </ol>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/docker/#in-production","title":"In Production","text":"<p>In the production environment, my app was running in kubernetes, so I loaded the env vars in through a kubernetes yaml manifest from secrets I stored in my cluster by using <code>kubectl create secret</code></p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/golang/","title":"Golang","text":"<p>The main reason for this project was to learn more about Go and familiarise how to create API resources so I can implement them for microservices architecture.</p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/golang/#echo-apis","title":"Echo (APIs)","text":"<p>The backend uses the Echo framework to create API resources that execute handler functions when called from the frontend.</p> <p>Some API resources are protected by putting them in restricted groups, that require a valid JWT token:</p> <pre><code>apiRestricted := e.Group(\"/api/v1/restricted\")\n\napiRestricted.Use(echojwt.WithConfig(echojwt.Config{\n        SigningKey:  hmacSecret,\n        TokenLookup: \"header:Authorization\",\n    }))\n</code></pre> <p>APIs in the backend are responsible for things such as:</p> <ul> <li>Updating prayer times to the frontend daily</li> <li>Calling databases to register new users and handle logins</li> <li>Setting cookies on the browser</li> </ul>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/golang/#middleware","title":"Middleware","text":"<p>The backend uses the following middleware</p> <ol> <li>Recover - recovers the server if it panics and terminates.</li> <li>Secure - provides security against cross-site scripting (XSS) attacks</li> <li>CORS - allows cross origin requests from only specific URLs</li> <li>JWT middleware - checks if JWT tokens are legit</li> </ol>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/golang/#handler-functions","title":"Handler functions","text":"<p>Theres a lot of different types of logic within the handler functions, here are some highlights, but they have their own pages under the Skills Used In Project section:</p> <ol> <li>Encrypting and decrypting users password using bcrypt.</li> <li>Creating JWT tokens</li> <li>Setting Cookies in the browser</li> <li>Getting/Posting data to Redis database/SQL postgres DB</li> <li>Calling 3rd party APIs</li> <li>Automatically sending emails using the smtp Go package.</li> <li>Type safety through parsing data into structs</li> <li>Error handling and sending correct API status codes/ data in body of API requests to frontend</li> </ol>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/kubernetes/","title":"Kubernetes","text":"<p>The application is running in a kubernetes cluster. Theres a lot of information I can dump here but I've actually documented it all in the Kubernetes section of this website.</p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/loginSystem/","title":"My Login system","text":"<p>One of the key things when creating the web app was designing a login system, which also including sanitising user input on the frontend to make sure valid email addresses &amp; strong passwords were being used. In the backend, it was pretty much 90% CRUD calls to database tables. Heres how it worked: </p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/loginSystem/#1-user-registering","title":"1. User Registering","text":"<p>When a user initially registers, it saves information about the user in a database and sends a verification code to the users email address. Here's what the database stores</p> <ul> <li>User's username (email) (primary key of DB in <code>users</code> table)</li> <li>hashed version of their password (bcrypt hashing)</li> <li>verification flag (checks to see if user has verified email, initially set to false when user registers)</li> <li>verification code expiry time</li> </ul> <p>The code below shows how the initial registration is setup:</p> <pre><code>var incomingUserRegistration UserCredentials\n    if err := c.Bind(&amp;incomingUserRegistration); err != nil {\n        return c.JSON(http.StatusBadRequest, map[string]string{\"error\": \"Invalid request body for new user registration\"})\n    }\n    hashed_password, err := hashPassword(incomingUserRegistration.UserPassword)\n    if err != nil {\n        logger.Errorf(fmt.Sprintf(\"Failed to hash password, err: %s\", err.Error()))\n        return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Failed to hash password\"})\n    }\n\n    incomingUserRegistration.UserPassword = hashed_password\n\n    // get current timestamp in postgres format to register when new user was created, can use to delete values\n    currentTime := currentTimeStampPostgres()\n    // set email verification for users initially to false when user is created, they need to confirm later\n    verifiedEmail := false\n\n    insertSQL := `\n    INSERT INTO users (\n        user_id, password_hash, creation_timestamp, verified_email\n    ) VALUES (\n        $1, $2, $3, $4\n    )\n`\n\n    _, err = db.Exec(insertSQL, incomingUserRegistration.UserEmail, incomingUserRegistration.UserPassword, currentTime, verifiedEmail)\n    if err != nil {\n        logger.Errorf(\"Failed to execute database sql statement, err: %w\", err)\n        return c.JSON(http.StatusAlreadyReported, map[string]string{\"error\": \"Failed to upload user data to server, is the email already in use?\"})\n    }\n\n    // add send email verification function here before returning registered user?\n    // generate random passphrase for email verification confirmation\n    verificationCode, err := generateRandomCode(6)\n    if err != nil {\n        return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Could not generate random code\"})\n    }\n\n    expiryTime := currentTimePlusHourPostgres()\n\n    insertVerificationCodeSQL := `\n    INSERT INTO email_verification_check (\n        user_id, email_verification_code, expiry_time\n    ) VALUES (\n        $1, $2, $3\n    );\n    `\n    _, err = db.Exec(insertVerificationCodeSQL, incomingUserRegistration.UserEmail, verificationCode, expiryTime)\n    if err != nil {\n        logger.Errorf(\"Failed to insert email verification code in DB, err %w\", err)\n        return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Failed to upload email verification code to db\"})\n    }\n\n    err = sendEmailVerification(c, verificationCode, incomingUserRegistration.UserEmail, logger)\n    if err != nil {\n        logger.Errorf(\"Error sending email, err: %s\", err.Error())\n        return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Send email verification failed\"})\n    }\n    logger.Info(\"email should have sent to user\")\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/loginSystem/#2-verify-user","title":"2. Verify User","text":""},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/loginSystem/#sending-the-user-a-verification-code","title":"Sending the user a verification code","text":"<p>Once the user successfully completes the initial registration step, the page then changes to a verify user page. The user must enter the verification code sent to them within the expiry time frame for the token (1 hour), below is the code snippet that does this.</p> <pre><code>func handleCreateUser(c echo.Context, logger *zap.SugaredLogger, db *sql.DB) error {\n    // this function parses the incoming user data, calls an encryption on the password then uploads it to the db\n    var incomingUserRegistration UserCredentials\n    if err := c.Bind(&amp;incomingUserRegistration); err != nil {\n        return c.JSON(http.StatusBadRequest, map[string]string{\"error\": \"Invalid request body for new user registration\"})\n    }\n    hashed_password, err := hashPassword(incomingUserRegistration.UserPassword)\n    if err != nil {\n        logger.Errorf(fmt.Sprintf(\"Failed to hash password, err: %s\", err.Error()))\n        return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Failed to hash password\"})\n    }\n\n    incomingUserRegistration.UserPassword = hashed_password\n\n    // get current timestamp in postgres format to register when new user was created, can use to delete values\n    currentTime := currentTimeStampPostgres()\n    // set email verification for users initially to false when user is created, they need to confirm later\n    verifiedEmail := false\n\n    insertSQL := `\n    INSERT INTO users (\n        user_id, password_hash, creation_timestamp, verified_email\n    ) VALUES (\n        $1, $2, $3, $4\n    )\n`\n\n    _, err = db.Exec(insertSQL, incomingUserRegistration.UserEmail, incomingUserRegistration.UserPassword, currentTime, verifiedEmail)\n    if err != nil {\n        logger.Errorf(\"Failed to execute database sql statement, err: %w\", err)\n        return c.JSON(http.StatusAlreadyReported, map[string]string{\"error\": \"Failed to upload user data to server, is the email already in use?\"})\n    }\n\n    // generate random passphrase for email verification confirmation\n    verificationCode, err := generateRandomCode(6)\n    if err != nil {\n        return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Could not generate random code\"})\n    }\n\n    expiryTime := currentTimePlusHourPostgres()\n\n    insertVerificationCodeSQL := `\n    INSERT INTO email_verification_check (\n        user_id, email_verification_code, expiry_time\n    ) VALUES (\n        $1, $2, $3\n    );\n    `\n    _, err = db.Exec(insertVerificationCodeSQL, incomingUserRegistration.UserEmail, verificationCode, expiryTime)\n    if err != nil {\n        logger.Errorf(\"Failed to insert email verification code in DB, err %w\", err)\n        return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Failed to upload email verification code to db\"})\n    }\n\n    err = sendEmailVerification(c, verificationCode, incomingUserRegistration.UserEmail, logger)\n    if err != nil {\n        logger.Errorf(\"Error sending email, err: %s\", err.Error())\n        return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Send email verification failed\"})\n    }\n\n\n    return c.JSON(http.StatusOK, map[string]string{\"error\": \"\"})\n}\n</code></pre> <p>I also added in the option to resend a verification code, where the calls to the DB were to delete all existing verification codes for the user, and then doing the same logic of writing a new verification code to the db and sending the user the new one.</p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/loginSystem/#user-submits-the-verification-code","title":"User submits the verification code","text":"<p>The user can then submit the verification code along with the email they signed up with, these values are checked against the DB and if they are valid, the <code>email_verification_check</code> flag is then updated to true.</p> <p>Note: I did realise later that checking if the expiry time is valid in the SQL would have made my code a lot cleaner...</p> <pre><code>CheckRecordExistsQuery := `\nSELECT user_id, email_verification_code, expiry_time\nFROM email_verification_check\nWHERE user_id = $1\nAND email_verification_code = $2\n`\nrows, err := db.Query(CheckRecordExistsQuery, EmailVerificationDetailsFromFrontend.UserEmail, verificationCodeFromFrontend)\nif err != nil {\n    logger.Error(\"Error in quering db for verification Email information\")\n    logger.Error(err)\n    return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Failed to run query for email verification on DB\"})\n}\n\ndefer rows.Close()\n\nvar countReturnedRows int\nvar EmailVerificationDBResults EmailVerificationDBResults\n\nfor rows.Next() {\n    err := rows.Scan(&amp;EmailVerificationDBResults.UserEmail, &amp;EmailVerificationDBResults.VerificationCode, &amp;EmailVerificationDBResults.ExpiryTime)\n    if err != nil {\n        logger.Errorf(\"Error in rows.Scan for parsing rows into EmailVerificationDBResults\")\n        return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Error in rows.Scan for parsing rows into EmailVerificationDBResults\"})\n    }\n\n    countReturnedRows++\n\n}\n\nif countReturnedRows &gt; 1 {\n    return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Rows returned more than 1 from DB for email verification check should only be one\"})\n}\n\nexpiryTimeValid := time.Now().Before(EmailVerificationDBResults.ExpiryTime)\n\n// if verificationcode from db is 0 then it is because there is no result so it is a default value, so check to see if not 0\nif EmailVerificationDBResults.VerificationCode == verificationCodeFromFrontend &amp;&amp; (EmailVerificationDBResults.VerificationCode != 0) &amp;&amp; expiryTimeValid {\n    // update verification flag in user database\n\n    updateVerificationFlag := `\n    UPDATE users\n    SET verified_email =  $1\n    WHERE user_id = $2\n    `\n\n    _, err := db.Exec(updateVerificationFlag, true, EmailVerificationDBResults.UserEmail)\n\n    if err != nil {\n        logger.Error(\"Failed to set email verification flag to true in DB\")\n        return c.JSON(http.StatusInternalServerError, map[string]string{\"error\": \"Failed to set email verification flag to true in DB\"})\n    }\n    return c.JSON(http.StatusOK, map[string]string{\"error\": \"\"})\n}\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/loginSystem/#3-logging-in","title":"3. Logging in","text":"<p>When the user logs in the db checks for 2 things:</p> <ol> <li>If the user has verified their email by checking boolean value of <code>verified_email</code> in the <code>users</code> table.</li> <li>If the hashed password from the user matches the hashed password stored in the DB (no passwords are stored in plaintext)</li> </ol> <p>If both are correct, the user is allowed to login, and a cookie is set in the browser with a JWT token. This JWT Token is used to access protected API routes that would not be accessible if the user did not have the correct JWT token stored. More about JWTs and how I implemented them can be found here!</p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/nextJs/","title":"Next JS","text":"<p>I used the Next JS framework for frontend, specifically the pages routing type, storing my api calls serverside under the <code>/api</code> folder that called the backend.</p> <p>Heres an example of some serverside code in Next JS I wrote that sets the cookie in the browser after making an API POST request to the Go server to let the user log in.</p> <pre><code>import { NextApiRequest, NextApiResponse } from \"next\";\n\nexport default async function PostLoginUser(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  if (req.method === \"POST\") {\n    const { userEmail, userPassword } = req.body;\n\n    try {\n      const response = await fetch(\"http://tpm-backend:80/api/v1/login\", {\n        method: \"POST\",\n        headers: {\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify({\n          userEmail,\n          userPassword,\n        }),\n        credentials: \"include\",\n      });\n\n      if (response.status == 200) {\n        const dataRes = await response.json();\n        const jwtCookie = response.headers.get(\"set-cookie\");\n        // Set the \"jwt\" cookie in the API route's response headers\n        if (jwtCookie) {\n          res.setHeader(\"Set-Cookie\", jwtCookie);\n        }\n\n        res.status(200).json(dataRes);\n      } else {\n        // If response is not successful, parse JSON response to get error message and status code\n        const errorResponse = await response.json();\n        res.status(500).json(errorResponse);\n      }\n    } catch (error) {\n      console.error(\"Error creating user:\", error);\n      res.status(500).json({ error: \"Internal server error\" });\n    }\n  } else {\n    res.status(405).json({ error: \"Method Not Allowed\" });\n  }\n}\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/passwordEncryption/","title":"Learning how to store passwords","text":"<p>I used a hashing algorithm called bcrypt, which hashed user passwords. I then stored those hashes in the database, instead of storing passwords in plaintext.</p> <p>By doing so, when the user logged in, I could hash the password they were using and compare it to the hashed password stored in the database. If the passwords were the same, I knew the password was correct, so could continue with the authentication process.</p> <p>Here are the go functions used:</p>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/passwordEncryption/#hash-password","title":"Hash password","text":"<pre><code>func hashPassword(password string) (string, error) {\n    bytes, err := bcrypt.GenerateFromPassword([]byte(password), 14)\n    return string(bytes), err\n}\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/passwordEncryption/#check-password-hash","title":"Check password hash","text":"<pre><code>func checkPasswordHash(password string, hash string) bool {\n    err := bcrypt.CompareHashAndPassword([]byte(hash), []byte(password))\n    return err == nil\n}\n</code></pre>"},{"location":"myProjects/theProductiveMuslim/skillsUsedInProject/typescript/","title":"Typescript","text":"<p>I dont particularly enjoy frontend coding, but I wrote my frontend code in typescript, as it helps reduce bugs and errors to do with data types.</p> <p>Heres an example of a function that I wrote that calculates what the current prayer is.</p> <pre><code>interface PrayerData {\n  Asr: string;\n  Dhuhr: string;\n  Fajr: string;\n  Isha: string;\n  Maghrib: string;\n}\n\ninterface ClosestPrayer {\n  name: string;\n  time: string;\n  difference: number;\n}\n\n// this function returns next prayer in closestPrayer interface\nasync function getCurrentPrayer(\n  todaysPrayers: PrayerData\n): Promise&lt;ClosestPrayer&gt; {\n  const currTime = new Date();\n\n  const filteredPrayerObj: Record&lt;string, ClosestPrayer&gt; = Object.entries(\n    todaysPrayers\n  ).reduce((acc, [key, value]) =&gt; {\n    const prayerTime = new Date(value);\n\n    if (prayerTime &lt;= currTime) {\n      const difference = currTime.getTime() - prayerTime.getTime();\n      acc[key] = { name: key, time: value, difference };\n    }\n\n    return acc;\n  }, {} as Record&lt;string, ClosestPrayer&gt;);\n  console.log(filteredPrayerObj);\n\n  // If there are no upcoming prayers, return null\n  if (Object.keys(filteredPrayerObj).length === 0) {\n    return {\n      name: \"null - no upcoming prayers\",\n      time: \"null\",\n      difference: 0,\n    };\n  }\n\n  const closestPrayer = Object.values(filteredPrayerObj).reduce(\n    (closest, current) =&gt; {\n      return current.difference &lt; closest.difference ? current : closest;\n    },\n    { name: \"\", time: \"\", difference: Infinity }\n  );\n\n  return closestPrayer;\n}\n\nexport default getCurrentPrayer;\n</code></pre>"}]}