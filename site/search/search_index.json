{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"about/","title":"Arenti tollens praedae nam nec quas cuspidis","text":""},{"location":"about/#exire-amplexus-oves-concurrere-simul-et-patres","title":"Exire amplexus oves concurrere simul et patres","text":"<p>Lorem markdownum me caelum negare semper vulnera inexcusabile illa capit. Et saevaque gelidis, potest maerenti quisquis sublime licet, flumina urbe aetherioque doleamne nurusque.</p> <p>Favilla solet quotiens ferarum, crinem, erat heros conatibus in fuderat tibique quod nec nepotem; qui Procne. Quiescet fert dixit et praestat, luctus induco. Mea tum inhaeserat, priori Achaidas, hanc lyra vocesque ferenda, una ecce numina, nati. Vidisti spatiosa dexteriore et membra vir certe loco quinque satis Berecyntia mihi et oculos. Movit illi fit in putat lino Iuppiter omnes nomen animo, quaque.</p>"},{"location":"about/#sit-si-pudorem-atras-sollicitatque-puro-vetito","title":"Sit si pudorem atras sollicitatque puro vetito","text":"<p>Subire videntur se ultra Calydonia responsaque in hoc auro, doloris magna. Secretaque volet maxime, habebat primus et cognoscere, ense Tartara elige Phoebeius non corpore quae.</p> <p>Sublimis ante pectora est, telae clamat tamen ut vetustas. Dumque sola tempore et Lyciae turba contremuit incursu nam est pullo aethere. Vobis faciat habeat equidem arcus, igni redit videntur ostendens culpae? Contentus iamque quaeque moras, oraque insidias digitis certamen conscia amplexus, commenta duritia Iovis.</p>"},{"location":"about/#occupat-nec-vel","title":"Occupat nec vel","text":"<p>Patulis est nomine per inrigat incola iacit indignatur casus, doleas et agris. Distantes pro sua aetas illi daedalus. Taurus nullique taurorum habenas; materia praereptaque pleni, quales e carmen frustra inopi inducere.</p> <p>Nec meminisse cladis, tot lumina retinere; habet neve inperfectus latet. Sole vidi sed quoque origo, tacuit; dum et et merito verba. Et cognita condicione chlamydem iurgia imagine, ad iterum remansit sparsit vix medio sumptoque quoque arbusta. Parsque carmine secuit, letalibus labor quam potest iactate illa, illo indicium.</p>"},{"location":"about/#his-querentes-decebat-suspenderat-parte-paulatimque-in","title":"His querentes decebat suspenderat parte paulatimque in","text":"<p>Domo absens, neu dedit, dare visae metitur an. Quidem prece.</p> <ol> <li>Etiam alite ut pectus inquit de Iuppiter</li> <li>Phoebus credas</li> <li>Inquit annis ille</li> <li>Illo puerpera</li> </ol> <p>Ut dedit praemia primus, Iove Inachus resoluta et operum. Pondere deae ageret fulvis Pyrrha indigenis longa non cohaeserat, quae suis tuque Erycina erit sine istum querella.</p>"},{"location":"questions/","title":"questions","text":"<ol> <li>what is the command to list namespaces and select one as default one? can only find config related commands online</li> </ol>"},{"location":"kubernetes/components/","title":"Kubernetes Component Documentation","text":""},{"location":"kubernetes/components/#worker-nodes","title":"worker nodes","text":"<p>When you deploy kubernetes you get a cluster, within a cluster you will get a set of machines, which are called worker nodes, which will run your application.</p> <p>These worker nodes will run pods inside them, which are your containers at run time. A worker node can have multiple applications running on it.</p> <p>There are three components each worker node is comprised of:</p> <ul> <li>kubelet - An agent that runs on each worker node in a cluster, it makes sure that containers are running on a pod and are healthy, its primary responsibility is to manage the containers scheduled on the pods. It also is responsible to schedule to containers to run on the pods.   Because kubelets are responsible for starting the pods, they have to interface with both the worker node resources e.g. cpu/ram and the container.</li> <li>container runtime - each node needs a container runtime installed so containers are able to be run there, there are different types of container runtimes e.g. containered/ docker engine.</li> <li>kube-proxy - this part includes a quick explanation of services and how they interact with kube-proxy. Because pods are ephemeral, their connection details can and will change. Due to this, we need a stable way to connect to the pods that run our containers. Enter Services. Services are logical abstractions which create stable endpoints to connect to the pods, we can define which pods we want the endpoints to expose connectivity to by setting them in <code>selector</code> part of the yaml. Doing so will expose those pods to be connected to from anywhere in the cluster. When network packets from the client hit the service, the service then distributes the network packet to one of the nodes which hold a pod that is tagged with the same <code>selector</code> value e.g. our <code>tpm-backend</code>. It does this based on an algorithm as to not cause any networking bottlenecks, e.g. it might use round-robin to send different network packets to different nodes. Once the network packet is at the node, the kube-proxy then chooses which pod to distribute that data to (as we could have multiple pods spun up for the same application to increase availability), it will make this decision based on multiple factors e.g. pod health and load balancing algorithms in place.</li> </ul>"},{"location":"kubernetes/components/#ingress","title":"Ingress","text":"<ul> <li>ingress controllers run the ingress yaml rules, they are not defaultly contained and started within the cluster, your preferred ingress controller (e.g. nginx) needs to be added to the cluster, so it can run the ingress rules defined</li> </ul>"},{"location":"kubernetes/kindCommands/","title":"Kind Commands","text":"<p>Kind is a tool used to run Kubernetes clusters inside Docker containers for local development and testing purposes.</p>"},{"location":"kubernetes/kindCommands/#get-cluster-names-in-kind","title":"Get cluster names in kind","text":"<p><code>kind get clusters</code></p>"},{"location":"kubernetes/kindCommands/#load-docker-images-into-kind","title":"Load docker images into kind","text":"<p><code>kind load docker-image &lt;Image name&gt; -n &lt;cluster name&gt;</code></p>"},{"location":"kubernetes/settingUpKubernetes/","title":"Kubernetes","text":"<p>Kubernetes is an open source system used to deploy, scale and manage containerised applications anywhere.</p> <p>It is great for the following reasons:</p> <ol> <li>It abstracts away infrastruture, as it handles compute/network/storage on behalf of your workloads.</li> <li>It has built in service health monitoring, restarting containers if they are unhealthy or fail, making sure you have high availability.</li> <li>It has built in commands to do a lot of heavy lifting that goes into application management, using <code>kubectl</code>.</li> </ol>"},{"location":"kubernetes/settingUpKubernetes/#components-of-kubernetes","title":"Components of Kubernetes","text":""},{"location":"kubernetes/settingUpKubernetes/#cluster","title":"Cluster","text":"<p>This is what all your kubernetes parts run in.</p>"},{"location":"kubernetes/settingUpKubernetes/#namespaces","title":"Namespaces","text":"<p>Namespaces conceptually compartmentalise your resources into virtual clusters, that way you can allocate a set amount of resources to each namespace. For example, if you create a namespace for each department in your company, you can limit the amount of CPU/RAM that namespace has to distribute amongst the components inside it.</p> <p>Once you have a namespace, you can go ahead and add resources to it, such as variables using configMaps and secrets, or RAM/CPU set up from the resource part of the deployment manifest (the deployment.yaml).</p>"},{"location":"kubernetes/settingUpKubernetes/#creating-a-namespace","title":"Creating a namespace","text":"<p><code>kubectl create namespace &lt;namespace name&gt;</code></p>"},{"location":"kubernetes/settingUpKubernetes/#get-all-namespaces","title":"Get all namespaces","text":"<p><code>kubectl get ns</code></p>"},{"location":"kubernetes/settingUpKubernetes/#configmaps","title":"ConfigMaps","text":"<p>ConfigMaps are objects used to store non-confidential variables as key : value pairs, pods can consume ConfigMaps as env variables/ comand line args or as config files in a volume. The advantage of ConfigMaps is that it allows you to decouple environment specific configurations from your application, making them easily portable and adjustable.</p>"},{"location":"kubernetes/settingUpKubernetes/#creating-configmaps-from-kubectl","title":"Creating ConfigMaps from kubectl","text":"<p><code>kubectl -n &lt;namespace name&gt; create configmap &lt;configmap name&gt; --from-literal ENV_VAR_NAME=value</code></p>"},{"location":"kubernetes/settingUpKubernetes/#list-out-configmaps","title":"list out configmaps:","text":"<p><code>kubectl -n &lt;namespace name&gt; get cm</code></p>"},{"location":"kubernetes/settingUpKubernetes/#output-configmaps-as-yaml","title":"Output configmaps as yaml","text":"<p><code>kubectl -n &lt;namespace name&gt; get cm &lt;configmap name&gt; -o yaml</code></p>"},{"location":"kubernetes/settingUpKubernetes/#secrets","title":"Secrets","text":"<p>Secrets are similar to configMaps except they are encrypted at rest. It is important to keep secrets separate from configMaps as you can apply rbac to give access to secrets to some people but not others.</p>"},{"location":"kubernetes/settingUpKubernetes/#creating-a-secret-from-kubectl","title":"Creating a secret from kubectl","text":"<pre><code>kubectl -n &lt;namespace name&gt; create secret generic &lt;secret name&gt; \\\n--from-literal SECRET_KEY=secretValue \\\n--from-literal SECRET_KEY_2=secret_value_2\n</code></pre>"},{"location":"kubernetes/settingUpKubernetes/#setting-secrets-in-deployment","title":"Setting secrets in deployment","text":"<p>Once you have created the secret, you can then pass the secret to your deployment yaml manifest like so:</p> <pre><code>    spec:\n      containers:\n        - name: tpm-backend\n          image: tpm-backend:0.0.2\n          imagePullPolicy: Never\n          ports:\n          - containerPort: 80\n          env:\n          - name: SECRET_NAME_IN_CONTAINER\n            valueFrom:\n                secretKeyRef:\n                    name: SECRET_NAME\n                    key: SECRET_KEY\n</code></pre>"},{"location":"kubernetes/settingUpKubernetes/#list-all-secrets","title":"List all secrets","text":"<p><code>kubectl -n &lt;namespace name&gt; get secret</code></p>"},{"location":"kubernetes/settingUpKubernetes/#deployments","title":"Deployments","text":"<p>We can use kubectl to apply our deployment manifest files. Deployment manifests are used to describe the specification of how we want to deploy our applications. e.g. what container, how much replicas we want, and the amount of resources we want to allocate to each deployment.</p> <p>Example of deployment manifest yaml file:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: mycontainer\n        image: myimage:tag\n        env:\n        - name: ENV_VARIABLE_FROM_CONFIGMAP\n          valueFrom:\n            configMapKeyRef:\n              name: myconfigmap\n              key: configKey\n        - name: ENV_VARIABLE_FROM_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: secretKey\n        ports:\n            - containerPort: 8080\n                protocol: TCP\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n</code></pre>"},{"location":"kubernetes/settingUpKubernetes/#deploy-yaml-deployment-manifest","title":"Deploy yaml deployment manifest","text":"<p><code>kubectl -n &lt;namespace name&gt; apply -f &lt;deployment.yaml file&gt;</code></p>"},{"location":"kubernetes/settingUpKubernetes/#view-deployed-pods","title":"View deployed pods","text":"<p>Get all pod names:</p> <p><code>kubectl -n &lt;namespace name&gt; get pods</code></p> <p>get specific pod:</p> <p><code>kubectl -n &lt;namespace name&gt; get pods/&lt;pod name&gt;</code></p>"},{"location":"kubernetes/settingUpKubernetes/#services","title":"Services","text":"<p>Services are logical abstractions that provide stable endpoint to access our pods. Because pods are ephemeral, their IP addresses change when being spun up and down. Due to this, we cannot directly connect to them, as the connection will be unreliable, to counteract this issue, we use services, where we can use a stable IP address and port to point to specific pods via the app name defined in the service.</p> <p>Pods running different applications can communicate with each other through services as long as each application is being exposed by a service.</p> <p>Services are defined through a yaml manifest that can be applied.</p> <p>Example of backend service:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: tpm-backend\nspec:\n  type: ClusterIP\n  selector:\n    app: tpm-backend\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n</code></pre> <p>In the example above <code>port</code> is the port on which the service itself listens for traffic, while <code>targetPort</code> is the port to which traffic is forwarded to the backend pods, hence the targetPort needs to be the same port that the application's deployment yaml is exposing.</p> <p>By default, the type of service is <code>ClusterIP</code> if no other type is set, this is an internal IP address that provides an IP address for other components within the cluster to connect to, but does not allow connection from anything external.</p>"},{"location":"kubernetes/settingUpKubernetes/#apply-service-yaml-manifest","title":"Apply service Yaml manifest","text":"<p><code>kubectl -n &lt;namespace name&gt; -f &lt;path to service yaml file&gt;</code></p>"},{"location":"kubernetes/settingUpKubernetes/#list-all-services-in-namespace","title":"list all services in namespace","text":"<p><code>kubectl -n &lt;namespace name&gt; get svc</code></p> <p>running the above command will give us information for each service, including the clusterIP we can access our deployment on, as well as the port it exposes.</p>"},{"location":"kubernetes/settingUpKubernetes/#applications-communicating-through-services-internally","title":"Applications communicating through services internally","text":"<p>If you have two different deployments running different images and each deployment exposes a service, the pods managed by these deployments can communicate with each other using their internal Cluster IP addresses and exposed ports.</p> <p>Here's how it works:</p> <p>1.You have two Deployments, each deploying Pods running different container images.</p> <p>2.Each Deployment exposes a Service, which creates a stable endpoint for accessing the Pods.</p> <p>3.The Pods can communicate with each other using the DNS name of the Service, which resolves to the internal Cluster IP address of the Service. They can use the internal Cluster IP address and exposed ports to send and receive data.</p> <p>Note that because services are logical abstractions, they can communicate across namespaces without any issues.</p>"},{"location":"kubernetes/settingUpKubernetes/#ingress","title":"Ingress","text":"<p>Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. You can define an ingress API object through a yaml file.</p> <p>In an ingress API object you define the host that you expect traffic to be incoming from, and can define where you want to route that traffic, by defining which service you want to send it to, by giving it the name and port of the service.</p> <p>Example of ingress file:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tpm-backend-ingress\nspec:\n  rules:\n    - host: \"foo-bar.com\" # //hostname for the application e.g. my website, need to map domain name to node IP address 4:45 on video\n      http:\n        paths:\n          - pathType: Prefix\n            path: \"/tpm\"\n            backend:\n              service:\n                name: tpm-backend\n                port:\n                  number: 8080 # this points to the service port we are exposing\n</code></pre> <p>Defining and deploying an ingress resource is not enough, we need an ingress controller to actually run the ingress resource.</p>"},{"location":"kubernetes/settingUpKubernetes/#ingress-controllers","title":"Ingress Controllers","text":"<p>Ingress controllers are used to run your ingress resource you have deployed through a yaml configuration. Controllers are not defaultly installed in your cluster, so you must install one manually, there are multiple resources to choose from, but we will keep it simple and use nginx.</p>"},{"location":"kubernetes/settingUpKubernetes/#deploy-nginx-ingress-controller-through-kubectl","title":"deploy Nginx ingress controller through kubectl","text":"<p>The first thing is deploying your ingress controller to manage ingress as per your ingress manifest.</p> <p><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.0/deploy/static/provider/cloud/deploy.yaml</code></p> <p>The command above will create an nginx controller within its own namespace <code>ingress-nginx</code>. The ingress controller normally goes into its own namespace to make it easier to manage its resources and also separate its own resources so it has the capability to manage its own traffic.</p> <p>Even though the ingress controller is in its own namespace, by default it listens out for all the ingress resources deployed in our cluster across all namespaces.</p> <p>Once you have an ingress controller deployed, you can then route external network traffic to the ingress controller by routing traffic from your DNS to the ingress controllers <code>loadBalancer</code> IP address.</p>"},{"location":"kubernetes/settingUpKubernetes/#get-loadbalancer-ip-address-of-ingress-controller","title":"Get LoadBalancer IP address of ingress controller.","text":"<p>This is the IP address you need to point external traffic to as an entry point to your cluster, i.e. you can set this IP address in the DNS settings of your cloudflare website. To find this IP address you can run the following command:</p> <p><code>kubectl -n &lt;ingress-controller-namespace&gt; get svc</code></p> <p>Note that this external IP address will be <code>pending</code> in the kind cluster, this is because LoadBalancer services are designed to use the load-balancer infrastructure your cloud provider offers, and since we aren't running in the cloud, there is none.</p> <p>This also means that when you apply your ingress manifest, there will be no address assigned, as the loadBalancer's external IP address will be the one assigned to your ingress.</p>"},{"location":"kubernetes/usefulCommands/","title":"useful commands","text":""},{"location":"kubernetes/usefulCommands/#describe-deployments","title":"Describe deployments","text":"<p>Gets deployment information</p> <p><code>kubectl describe deployment &lt;deployment name&gt;  -n &lt;namespace&gt;</code></p> <p>e.g.</p> <p><code>kubectl describe deployment tpm-backend  -n tpm</code></p>"},{"location":"kubernetes/usefulCommands/#retagging-existing-docker-image","title":"Retagging existing docker image","text":"<p><code>docker image tag tpm-backend:latest tpm-backend:0.0.1</code></p>"},{"location":"kubernetes/usefulCommands/#pushing-updated-code-to-kind-pod","title":"Pushing updated code to kind pod","text":"<ol> <li>rebuild your image with a new tag e.g.</li> </ol> <pre><code>-    docker build -t tpm-backend:0.0.2\n+    docker build -t tpm-backend:0.0.2\n</code></pre> <ol> <li>change deployment file to point image section to new tag</li> </ol> <pre><code>containers:\n        - name: tpm-backend\n-          image: tpm-backend:0.0.1\n+          image: tpm-backend:0.0.2\n</code></pre> <ol> <li>redeploy the deployment file using the following:</li> </ol> <p><code>kubectl apply -f &lt;path to deployment.yaml&gt;</code></p>"},{"location":"kubernetes/usefulCommands/#view-pod-status","title":"View pod status","text":"<p><code>kubectl get pods</code></p> <p><code>kubectl get pods/&lt;pod name&gt;</code></p>"},{"location":"kubernetes/usefulCommands/#view-container-logs-from-the-pod-its-running-in","title":"View container logs from the pod its running in","text":"<p><code>kubectl logs -f pod/tpm-backend-c5587bf9b-89pds -c cloud-sql-proxy</code></p> <p>-c stands for container <code>kubectl logs -f pod/&lt;pod name&gt; -c &lt;container name that is set in deployment.yaml&gt;</code></p>"}]}